{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5e49b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. è¯»å–æ•°æ®\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. æ–‡æœ¬é¢„å¤„ç†å‡½æ•°ï¼šåªä¿ç•™ \"ind_xxx\" è¿™æ ·çš„ç‰¹å¾å\n",
    "# ä¾‹å¦‚å°† \"ind_4e < 1 or missing\" è½¬åŒ–ä¸º \"ind_4e\"\n",
    "def preprocess_rule(text):\n",
    "    import re\n",
    "    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä»¥ ind å¼€å¤´çš„ç‰¹å¾å\n",
    "    features = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(features)\n",
    "\n",
    "# 3. åº”ç”¨é¢„å¤„ç†\n",
    "df['features_text'] = df['DetailedSplit'].apply(preprocess_rule)\n",
    "\n",
    "# 4. æ„å»ºçŸ©é˜µ (One-Hot Encoding)\n",
    "# binary=True è¡¨ç¤ºæˆ‘ä»¬åªå…³å¿ƒâ€œæœ‰æ²¡æœ‰ç”¨åˆ°è¿™ä¸ªç‰¹å¾â€ï¼Œä¸å…³å¿ƒç”¨äº†å‡ æ¬¡\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df['features_text'])\n",
    "\n",
    "# 5. è½¬åŒ–ä¸º DataFrame æŸ¥çœ‹ (è¿™å°±æ˜¯â€œè§„åˆ™-ç‰¹å¾â€çŸ©é˜µ)\n",
    "rule_feature_matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# å°†åŸå§‹çš„ Pointï¼ˆåˆ†æ•°ï¼‰æ‹¼å›æ¥ï¼Œæ–¹ä¾¿å¯¹ç…§\n",
    "rule_feature_matrix['Score_Points'] = df['Point']\n",
    "\n",
    "print(rule_feature_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45524a65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 0: å‡†å¤‡å·¥ä½œ\n",
    "# ==========================================\n",
    "# å‡è®¾è¿™æ˜¯ä½ æ‰€æœ‰çš„ 17 ä¸ªæŒ‡æ ‡åç§°ï¼ˆè¯·æŠŠè¿™é‡Œæ¢æˆä½ çœŸå®çš„åˆ—è¡¨ï¼‰\n",
    "# å³ä½¿æŸäº›æŒ‡æ ‡åœ¨ rules é‡Œä¸€æ¬¡éƒ½æ²¡å‡ºç°ï¼Œå®ƒä»¬ä¹Ÿä¼šä½œä¸ºå…¨ 0 åˆ—å‡ºç°åœ¨ç»“æœä¸­\n",
    "all_17_indicators = [\n",
    "    \"ind_4e\", \"ind_13b\", \"ind_3a_1\", \"ind_12f\", \"ind_2a_1\", \n",
    "    \"ind_13a_1\", \"ind_6\", \"ind_7\", \"ind_8\", \"ind_9\", \n",
    "    \"ind_10\", \"ind_11\", \"ind_14\", \"ind_15\", \"ind_16\", \n",
    "    \"ind_17\", \"ind_unused_example\" # ç¡®ä¿è¿™é‡Œåˆ—å‡ºäº†å…¨éƒ¨ 17 ä¸ª\n",
    "]\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 1: æ–‡æœ¬é¢„å¤„ç† (ä¿æŒ 210 è¡Œä¸å˜)\n",
    "# ==========================================\n",
    "# æˆ‘ä»¬åªæå–ç‰¹å¾åï¼Œä¸æ‹†åˆ†è¡Œ\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    # æå–æ‰€æœ‰ ind_ å¼€å¤´çš„è¯\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "df['feature_text'] = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 2: æ„å»ºçŸ©é˜µ (å¼ºåˆ¶ä½¿ç”¨æ‰€æœ‰ 17 ä¸ªæŒ‡æ ‡)\n",
    "# ==========================================\n",
    "# å…³é”®ç‚¹ï¼šä½¿ç”¨ vocabulary å‚æ•°ï¼\n",
    "# è¿™å‘Šè¯‰ç¨‹åºï¼šâ€œåªå…³æ³¨è¿™17ä¸ªè¯ï¼Œå…¶ä»–çš„æˆ‘ä¸è¦ï¼›æ²¡å‡ºç°çš„è¯ä¹Ÿè¦ç»™æˆ‘ç•™åˆ—ä½ç½®ã€‚â€\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=all_17_indicators)\n",
    "\n",
    "# ç”ŸæˆçŸ©é˜µ\n",
    "X = vectorizer.fit_transform(df['feature_text'])\n",
    "\n",
    "# è½¬åŒ–ä¸º DataFrameï¼Œåˆ—åå°±æ˜¯æˆ‘ä»¬æŒ‡å®šçš„é¡ºåº\n",
    "matrix_df = pd.DataFrame(X.toarray(), columns=all_17_indicators)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 3: æ‹¼æ¥ç»“æœ (åŠ ä¸Š Rule å’Œ Point)\n",
    "# ==========================================\n",
    "# axis=1 è¡¨ç¤ºå·¦å³æ¨ªå‘æ‹¼æ¥\n",
    "final_df = pd.concat([\n",
    "    df[['DetailedSplit', 'Point']],  # ç¬¬ä¸€åˆ—æ˜¾ç¤º Ruleï¼Œç¬¬äºŒåˆ—æ˜¾ç¤ºåˆ†æ•°\n",
    "    matrix_df                        # åé¢è·Ÿç€ 17 åˆ—æŒ‡æ ‡çŸ©é˜µ\n",
    "], axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 4: æ£€æŸ¥ä¸ä¿å­˜\n",
    "# ==========================================\n",
    "print(f\"æœ€ç»ˆçŸ©é˜µç»´åº¦: {final_df.shape}\") \n",
    "# é¢„æœŸè¾“å‡º: (210, 2 + 17) = (210, 19)\n",
    "\n",
    "# é¢„è§ˆä¸€ä¸‹\n",
    "print(final_df.head())\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "final_df.to_excel(\"final_rule_matrix.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45672fc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 0: å‡†å¤‡å·¥ä½œ\n",
    "# ==========================================\n",
    "# æ‚¨çš„ 17 ä¸ªæŒ‡æ ‡åç§° (è¯·ç¡®ä¿è¿™é‡Œæ˜¯å…¨çš„)\n",
    "all_17_indicators = [\n",
    "    \"ind_4e\", \"ind_13b\", \"ind_3a_1\", \"ind_12f\", \"ind_2a_1\", \n",
    "    \"ind_13a_1\", \"ind_6\", \"ind_7\", \"ind_8\", \"ind_9\", \n",
    "    \"ind_10\", \"ind_11\", \"ind_14\", \"ind_15\", \"ind_16\", \n",
    "    \"ind_17\", \"ind_unused_example\" \n",
    "]\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "# å‡è®¾æ‚¨çš„ç¬¬ä¸€åˆ—æ˜¯ Indexï¼Œæˆ‘ä»¬ç”¨ index_col=0 è¯»å–å®ƒï¼Œæˆ–è€…æŠŠå®ƒè¯»ä½œæ™®é€šåˆ—\n",
    "# è¿™é‡Œå»ºè®®è¯»ä½œæ™®é€šåˆ—ï¼Œæ–¹ä¾¿æˆ‘ä»¬ä¿å­˜åˆ°ç»“æœé‡Œ\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# ã€å…³é”®æ’æŸ¥ç‚¹ 1ã€‘æ‰“å°åˆšè¯»è¿›æ¥æ—¶çš„è¡Œæ•°\n",
    "print(f\"åŸå§‹æ•°æ®è¡Œæ•°: {df.shape[0]}\") \n",
    "# å¦‚æœè¿™é‡Œå·²ç»æ˜¯ 421ï¼Œè¯´æ˜ Excel æ–‡ä»¶æœ¬èº«å°±æ˜¯è„çš„ï¼ˆä¹‹å‰å­˜é”™äº†ï¼‰\n",
    "\n",
    "# å‡è®¾ç¬¬ä¸€åˆ—å« 'Index' (å¦‚æœä¸æ˜¯ï¼Œè¯·æŠŠ df.columns[0] æ”¹æˆæ‚¨çš„åˆ—å)\n",
    "index_col_name = df.columns[0] \n",
    "print(f\"æˆ‘ä»¬å°†ä½¿ç”¨åˆ— '{index_col_name}' ä½œä¸ºåŸå§‹ç´¢å¼•è¿½è¸ª\")\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 1: æ–‡æœ¬é¢„å¤„ç† (ç»å¯¹ä¸è¿›è¡Œæ‹†åˆ†)\n",
    "# ==========================================\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "df['feature_text'] = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 2: æ„å»ºçŸ©é˜µ\n",
    "# ==========================================\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=all_17_indicators)\n",
    "X = vectorizer.fit_transform(df['feature_text'])\n",
    "matrix_df = pd.DataFrame(X.toarray(), columns=all_17_indicators)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 3: æ‹¼æ¥ç»“æœ (å¸¦ä¸ŠåŸå§‹ Index)\n",
    "# ==========================================\n",
    "final_df = pd.concat([\n",
    "    df[[index_col_name, 'DetailedSplit', 'Point']], # æŠŠ Index åˆ—æ”¾æœ€å‰é¢\n",
    "    matrix_df\n",
    "], axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# æ­¥éª¤ 4: æ£€æŸ¥\n",
    "# ==========================================\n",
    "print(f\"æœ€ç»ˆçŸ©é˜µç»´åº¦: {final_df.shape}\")\n",
    "\n",
    "# å¦‚æœè¡Œæ•°ä¸å¯¹ï¼Œæˆ‘ä»¬é€šè¿‡ Index çœ‹çœ‹æ˜¯è°é‡å¤äº†\n",
    "if final_df.shape[0] > 210:\n",
    "    print(\"\\nè­¦å‘Šï¼šè¡Œæ•°å¼‚å¸¸å¢åŠ ï¼æ­£åœ¨æŸ¥æ‰¾é‡å¤çš„ Index...\")\n",
    "    duplicates = final_df[final_df.duplicated(subset=[index_col_name], keep=False)]\n",
    "    print(duplicates[[index_col_name, 'DetailedSplit']].head(10))\n",
    "    print(\"\\nå¦‚æœçœ‹åˆ°ä¸Šé¢çš„ Index æœ‰é‡å¤ï¼Œè¯´æ˜æ•°æ®æºé‡Œè¿™äº›è¡Œè¢«æ‹†åˆ†äº†ã€‚\")\n",
    "\n",
    "# ä¿å­˜\n",
    "final_df.to_excel(\"final_rule_matrix_with_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08670d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. è¯»å–çŸ©é˜µ\n",
    "df = pd.read_excel(\"final_rule_matrix.xlsx\")\n",
    "\n",
    "# 2. æå–åªæœ‰ 0/1 çš„ç‰¹å¾éƒ¨åˆ† (å‡è®¾ä»ç¬¬3åˆ—å¼€å§‹æ˜¯ç‰¹å¾)\n",
    "# æ‚¨çš„åˆ—æ˜¯: DetailedSplit, Point, ind_1, ind_2 ...\n",
    "feature_cols = df.columns[2:] \n",
    "X = df[feature_cols]\n",
    "\n",
    "# 3. ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(X, cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"Rule-Feature Heatmap (Dark Blue = Feature Used)\")\n",
    "plt.xlabel(\"Indicators\")\n",
    "plt.ylabel(\"Rule ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585af28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. è®¡ç®—èšç±»è¿æ¥çŸ©é˜µ (ä½¿ç”¨ Jaccard è·ç¦»ï¼Œé€‚åˆ binary æ•°æ®)\n",
    "# method='average' æˆ– 'complete' é€šå¸¸æ•ˆæœè¾ƒå¥½\n",
    "Z = linkage(X, method='average', metric='jaccard')\n",
    "\n",
    "# 2. ç»˜åˆ¶æ ‘çŠ¶å›¾ (å¸®åŠ©æ‚¨å†³å®šåˆ‡æˆå‡ ç±»)\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Rule Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.axhline(y=0.7, c='r', ls='--', lw=2) # ç”»ä¸€æ¡è¾…åŠ©çº¿ï¼Œçœ‹çœ‹åˆ‡åœ¨è¿™é‡Œä¼šåˆ†å‡ºå‡ ç±»\n",
    "plt.show()\n",
    "\n",
    "# 3. çœŸæ­£æ‰“æ ‡ç­¾ (å‡è®¾æˆ‘ä»¬æ ¹æ®æ ‘çŠ¶å›¾å†³å®šåˆ‡æˆ 8 ç±»)\n",
    "# t=8 è¡¨ç¤ºæˆ‘ä»¬è¦ 8 ä¸ªç°‡\n",
    "labels = fcluster(Z, t=8, criterion='maxclust')\n",
    "df['Cluster_Label'] = labels\n",
    "\n",
    "print(df['Cluster_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0568b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# å¯¹æ¯ä¸ªç°‡è¿›è¡Œèšåˆåˆ†æ\n",
    "cluster_profile = df.groupby('Cluster_Label')[feature_cols].mean()\n",
    "\n",
    "# åªè¦æŸä¸ªç‰¹å¾åœ¨è¯¥ç°‡çš„å‡ºç°ç‡è¶…è¿‡ 80% (0.8)ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå®ƒæ˜¯è¯¥ç°‡çš„â€œæ ¸å¿ƒç‰¹å¾â€\n",
    "for cluster_id in cluster_profile.index:\n",
    "    row = cluster_profile.loc[cluster_id]\n",
    "    core_features = row[row > 0.8].index.tolist()\n",
    "    \n",
    "    # è®¡ç®—è¯¥ç°‡çš„å¹³å‡é£é™©åˆ†\n",
    "    avg_score = df[df['Cluster_Label'] == cluster_id]['Point'].mean()\n",
    "    \n",
    "    print(f\"=== Cluster {cluster_id} (é£é™©åˆ†: {avg_score:.1f}) ===\")\n",
    "    print(f\"æ ¸å¿ƒç‰¹å¾: {core_features}\")\n",
    "    print(f\"è§„åˆ™æ•°é‡: {len(df[df['Cluster_Label'] == cluster_id])}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb36fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x=\"Cluster_Label\", y=\"Point\", data=df, jitter=0.2, size=5)\n",
    "plt.title(\"Score Distribution by Cluster\")\n",
    "plt.axhline(0, color='red', linestyle='--') # 0åˆ†çº¿\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9b276",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. è¯»å–æ•°æ® & å‡†å¤‡\n",
    "# ==========================================\n",
    "# è¯»å–ä¸Šä¸€æ­¥ç”Ÿæˆçš„çŸ©é˜µæ–‡ä»¶\n",
    "df = pd.read_excel(\"final_rule_matrix.xlsx\")\n",
    "\n",
    "# æå–ç‰¹å¾åˆ—ï¼ˆå‡è®¾ä»ç¬¬3åˆ— 'ind_xxx' å¼€å§‹æ˜¯ç‰¹å¾ï¼Œå‰ä¸¤åˆ—æ˜¯ DetailedSplit å’Œ Pointï¼‰\n",
    "# è¯·æ ¹æ®æ‚¨çš„å®é™…åˆ—åè°ƒæ•´ï¼Œç¡®ä¿ feature_cols åªåŒ…å« 0/1 çš„æŒ‡æ ‡åˆ—\n",
    "feature_cols = df.columns[2:] \n",
    "X = df[feature_cols]\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ‰§è¡Œèšç±» (å¤ç”¨ä¹‹å‰çš„é€»è¾‘)\n",
    "# ==========================================\n",
    "# ä½¿ç”¨ Jaccard è·ç¦»è¿›è¡Œå±‚æ¬¡èšç±»\n",
    "Z = linkage(X, method='average', metric='jaccard')\n",
    "\n",
    "# è®¾å®šèšç±»å‚æ•° (è¿™é‡Œæ¼”ç¤ºç”¨ distance = 0.7 è‡ªåŠ¨åˆ‡åˆ†ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ”¹ç”¨ t=8, criterion='maxclust')\n",
    "threshold = 0.7\n",
    "labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# å°†èšç±»ç»“æœæ‰“æ ‡åˆ°åŸå§‹æ•°æ®ä¸Š\n",
    "df.insert(0, 'Cluster_Label', labels) # æŠŠ Cluster_Label æ’åˆ°ç¬¬ä¸€åˆ—ï¼Œæ˜¾çœ¼\n",
    "\n",
    "# æŒ‰ ç°‡ID å’Œ åˆ†æ•°(Point) æ’åºï¼Œæ–¹ä¾¿é˜…è¯»\n",
    "df = df.sort_values(by=['Cluster_Label', 'Point'], ascending=[True, False])\n",
    "\n",
    "# ==========================================\n",
    "# 3. ç”Ÿæˆâ€œç°‡ç”»åƒâ€æ¦‚è§ˆ (Summary)\n",
    "# ==========================================\n",
    "summary_list = []\n",
    "unique_labels = sorted(df['Cluster_Label'].unique())\n",
    "\n",
    "for label in unique_labels:\n",
    "    # å–å‡ºè¯¥ç°‡çš„æ‰€æœ‰æ•°æ®\n",
    "    sub_df = df[df['Cluster_Label'] == label]\n",
    "    \n",
    "    # è®¡ç®—æ ¸å¿ƒç‰¹å¾ï¼šåœ¨è¯¥ç°‡ä¸­å‡ºç°é¢‘ç‡ > 80% çš„ç‰¹å¾\n",
    "    # mean() ä¼šå¯¹ 0/1 åˆ—æ±‚å‡å€¼ï¼Œå³å‡ºç°é¢‘ç‡\n",
    "    feat_freq = sub_df[feature_cols].mean()\n",
    "    core_feats = feat_freq[feat_freq > 0.8].index.tolist()\n",
    "    \n",
    "    summary_list.append({\n",
    "        'Cluster_Label': label,\n",
    "        'Rule_Count': len(sub_df),            # è§„åˆ™æ•°é‡\n",
    "        'Avg_Point': sub_df['Point'].mean(),  # å¹³å‡åˆ†\n",
    "        'Min_Point': sub_df['Point'].min(),   # æœ€ä½åˆ†\n",
    "        'Max_Point': sub_df['Point'].max(),   # æœ€é«˜åˆ†\n",
    "        'Core_Features': \", \".join(core_feats) # æ ¸å¿ƒç‰¹å¾åˆ—è¡¨\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# ==========================================\n",
    "# 4. ä¿å­˜åˆ° Excel (åŒ…å«ä¸¤ä¸ª Sheet)\n",
    "# ==========================================\n",
    "output_file = \"Fraud_Rules_Cluster_Report.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Sheet 1: æ¦‚è§ˆç”»åƒ\n",
    "    summary_df.to_excel(writer, sheet_name='Cluster_Summary', index=False)\n",
    "    \n",
    "    # Sheet 2: è¯¦ç»†æ•°æ® (æ•´è¡Œæ•°æ®éƒ½åœ¨è¿™é‡Œ)\n",
    "    df.to_excel(writer, sheet_name='Cluster_Details', index=False)\n",
    "\n",
    "print(f\"æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}\")\n",
    "print(\"Sheet 'Cluster_Summary': åŒ…å«æ¯ä¸ªç°‡çš„ç”»åƒç»Ÿè®¡\")\n",
    "print(\"Sheet 'Cluster_Details': åŒ…å«æŒ‰ç°‡å½’ç±»çš„æ‰€æœ‰åŸå§‹è§„åˆ™è¯¦æƒ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3fb82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# ==========================================\n",
    "# 1. è¯»å–åŸå§‹æ•°æ® & ä¿ç•™ Index\n",
    "# ==========================================\n",
    "# è¯»å– Excel\n",
    "# index_col=None ç¡®ä¿ç¬¬ä¸€åˆ—è¢«è¯»ä½œæ™®é€šåˆ—ï¼Œè€Œä¸æ˜¯ç´¢å¼•ï¼ˆé˜²æ­¢ä¸¢å¤±ï¼‰\n",
    "df = pd.read_excel(\"tree.xlsx\", sheet_name='Sheet1', index_col=None)\n",
    "\n",
    "# è·å–ç¬¬ä¸€åˆ—çš„åˆ—åï¼ˆå‡è®¾ç¬¬ä¸€åˆ—å°±æ˜¯æ‚¨è¯´çš„ Indexï¼‰\n",
    "index_col_name = df.columns[0]\n",
    "print(f\"æ£€æµ‹åˆ°åŸå§‹ Index åˆ—åä¸º: {index_col_name}\")\n",
    "\n",
    "# ä¸ºäº†é˜²æ­¢åç»­å¤„ç†ä¸¢å¤±ï¼Œæˆ‘ä»¬æ˜¾å¼åœ°æŠŠå®ƒé‡å‘½åä¸º 'Original_Index' (å¯é€‰ï¼Œä¹Ÿå¯ä¿ç•™åŸå)\n",
    "# è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™åŸåï¼Œä½†åœ¨è¾“å‡ºæ—¶è°ƒæ•´ä½ç½®\n",
    "\n",
    "# ==========================================\n",
    "# 2. å‡†å¤‡èšç±»ç‰¹å¾\n",
    "# ==========================================\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "# æå–ç‰¹å¾ç”¨äºè®¡ç®—\n",
    "rule_text = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# æ„å»ºçŸ©é˜µ\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(rule_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ==========================================\n",
    "# 3. æ‰§è¡Œèšç±» (è·ç¦»é˜ˆå€¼ 0.7)\n",
    "# ==========================================\n",
    "Z = linkage(X.toarray(), method='average', metric='jaccard')\n",
    "threshold = 0.7\n",
    "labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# ==========================================\n",
    "# 4. æ•´ç†ç»“æœè¡¨æ ¼\n",
    "# ==========================================\n",
    "# æ’å…¥ Cluster_Label åˆ°ç¬¬ 1 åˆ—\n",
    "if 'Cluster_Label' in df.columns:\n",
    "    df.drop(columns=['Cluster_Label'], inplace=True)\n",
    "df.insert(0, 'Cluster_Label', labels)\n",
    "\n",
    "# === å…³é”®æ­¥éª¤ï¼šè°ƒæ•´åˆ—é¡ºåº ===\n",
    "# æˆ‘ä»¬å¸Œæœ›é¡ºåºæ˜¯: Cluster_Label -> Original_Index -> Point -> å…¶ä»–åˆ—\n",
    "# å…ˆæŠŠ index åˆ—ç§»åˆ°ç¬¬ 2 åˆ—çš„ä½ç½® (ç´§è·Ÿ Cluster_Label)\n",
    "cols = list(df.columns)\n",
    "# ç§»é™¤ index åˆ—å’Œ Cluster_Label (é˜²æ­¢é‡å¤)\n",
    "cols.remove('Cluster_Label')\n",
    "cols.remove(index_col_name)\n",
    "# é‡æ–°ç»„åˆï¼šCluster_Label, Indexåˆ—, å…¶ä»–åˆ—...\n",
    "new_order = ['Cluster_Label', index_col_name] + cols\n",
    "df = df[new_order]\n",
    "\n",
    "# æŒ‰ ç°‡ID æ­£åºï¼Œåˆ†æ•° å€’åº æ’åº\n",
    "df_sorted = df.sort_values(by=['Cluster_Label', 'Point'], ascending=[True, False])\n",
    "\n",
    "# ==========================================\n",
    "# 5. ç”Ÿæˆç”»åƒæ¦‚è§ˆ\n",
    "# ==========================================\n",
    "X_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "X_df['Cluster_Label'] = labels\n",
    "\n",
    "summary_list = []\n",
    "unique_labels = sorted(df['Cluster_Label'].unique())\n",
    "\n",
    "for label in unique_labels:\n",
    "    sub_df = df[df['Cluster_Label'] == label]\n",
    "    \n",
    "    # ç”»åƒè®¡ç®—\n",
    "    sub_X = X_df[X_df['Cluster_Label'] == label]\n",
    "    feat_freq = sub_X.drop(columns=['Cluster_Label']).mean()\n",
    "    core_feats = feat_freq[feat_freq > 0.8].index.tolist()\n",
    "    \n",
    "    summary_list.append({\n",
    "        'Cluster_Label': label,\n",
    "        'Rule_Count': len(sub_df),\n",
    "        'Avg_Point': sub_df['Point'].mean(),\n",
    "        'Core_Features': \", \".join(core_feats),\n",
    "        # é¡ºä¾¿å±•ç¤ºè¯¥ç°‡é‡ŒåŒ…å«çš„å‡ ä¸ª Index æ ·ä¾‹ï¼Œæ–¹ä¾¿å¿«é€Ÿå®šä½\n",
    "        'Sample_Indices': str(sub_df[index_col_name].head(5).tolist()) + \"...\" \n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# ==========================================\n",
    "# 6. ä¿å­˜\n",
    "# ==========================================\n",
    "output_file = \"Fraud_Rules_Clustered_With_Index.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Cluster_Summary', index=False)\n",
    "    df_sorted.to_excel(writer, sheet_name='Cluster_Details', index=False)\n",
    "\n",
    "print(f\"æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}\")\n",
    "print(f\"Sheet 'Cluster_Details' çš„ç¬¬ 1 åˆ—æ˜¯ Cluster IDï¼Œç¬¬ 2 åˆ—æ˜¯åŸå§‹ {index_col_name}ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973a946",
   "metadata": {},
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Expert with 20 years of experience in financial institutions. You specialize in interpreting machine learning model rules (specifically XGBoost) to uncover underlying fraud patterns and business anomalies.\n",
    "\n",
    "# Task\n",
    "I will provide 210 fraud detection rules extracted from an XGBoost model. Each rule consists of multiple logical conditions based on specific Indicators and an associated risk score (Points). \n",
    "Your task is to perform **Semantic Clustering** on these 210 rules based on their **actual business meanings**.\n",
    "\n",
    "# Context & Data Dictionary\n",
    "Before analyzing, please study the following data dictionary carefully to understand what each indicator represents:\n",
    "\n",
    "\n",
    "# Requirements\n",
    "1. **Focus on Logic, Not Thresholds**: Do not separate rules just because of minor threshold differences (e.g., < 1 vs < 1.01). Focus on the **core business intent**.\n",
    "2. **Identify Feature Combinations**: Pay attention to indicators that frequently appear together (e.g., High Inventory + Over-financing).\n",
    "3. **Cluster into Scenarios**: Group these 210 rules into **8 to 12 distinct Risk Scenarios**.\n",
    "\n",
    "# Output Format\n",
    "Please provide the results in the following format for each cluster:\n",
    "\n",
    "## Cluster X: [Professional Business Name, e.g., Inventory Inflation & Over-financing]\n",
    "- **Risk Logic**: [A concise summary of the fraud/risk pattern. E.g., The entity maintains excessive inventory while simultaneously over-borrowing, suggesting potential loan fraud or asset overstatement.]\n",
    "- **Core Indicators**: [The primary 2-3 indicators defining this cluster.]\n",
    "- **Severity**: [High/Medium/Low based on the average Points.]\n",
    "- **Included Rule Indices**: [List all Rule Indices belonging to this group, e.g., 201, 208, 209...]\n",
    "\n",
    "# Data\n",
    "Below are the 210 rules:\n",
    "[Paste your CSV data here, including Index, Points, and DetailedSplit columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aff66e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_indicators(text):\n",
    "    \"\"\"ä» DetailedSplit æ–‡æœ¬ä¸­æå–æŒ‡æ ‡åç§° (ind_xxx)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… ind å¼€å¤´çš„æŒ‡æ ‡\n",
    "    # \\w+ åŒ¹é…å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿\n",
    "    return list(set(re.findall(r\"(ind[\\w_]+)\", str(text))))\n",
    "\n",
    "def generate_rule_tables(rule_lists_dict, master_df, index_col='Index'):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆè§„åˆ™æ±‡æ€»è¡¨(Table 1)å’Œè¯¦æƒ…è¡¨(Table 2)\n",
    "    \n",
    "    Args:\n",
    "        rule_lists_dict (dict): { 'List Name': [rule_index_1, rule_index_2, ...] }\n",
    "        master_df (pd.DataFrame): åŒ…å«æ‰€æœ‰è§„åˆ™çš„åŸå§‹æ•°æ®è¡¨\n",
    "        index_col (str): åŸå§‹æ•°æ®ä¸­ä»£è¡¨ Rule Index çš„åˆ—å\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (summary_df, details_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # å‡†å¤‡å·¥ä½œ\n",
    "    # ------------------------------------------\n",
    "    summary_data = []\n",
    "    details_data = []\n",
    "    \n",
    "    # ç¡®ä¿ç´¢å¼•åˆ—æ˜¯å­—ç¬¦ä¸²æˆ–ç»Ÿä¸€æ ¼å¼ï¼Œæ–¹ä¾¿åŒ¹é…\n",
    "    master_df[index_col] = master_df[index_col].astype(str)\n",
    "    \n",
    "    # éå†æ¯ä¸€ä¸ª List\n",
    "    for list_name, indices in rule_lists_dict.items():\n",
    "        # ç»Ÿä¸€è½¬ä¸ºå­—ç¬¦ä¸²åˆ—è¡¨\n",
    "        indices = [str(i) for i in indices]\n",
    "        \n",
    "        # 1. æå–è¯¥ List å¯¹åº”çš„æ‰€æœ‰è§„åˆ™å­é›†\n",
    "        # ä½¿ç”¨ isin å¿«é€Ÿç­›é€‰\n",
    "        subset = master_df[master_df[index_col].isin(indices)].copy()\n",
    "        \n",
    "        if subset.empty:\n",
    "            print(f\"Warning: List '{list_name}' é‡Œçš„ Index åœ¨åŸå§‹è¡¨ä¸­éƒ½æ‰¾ä¸åˆ°ï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "            \n",
    "        # ------------------------------------------\n",
    "        # ç”Ÿæˆ è¡¨äºŒ (Details) çš„æ•°æ®\n",
    "        # ------------------------------------------\n",
    "        # æˆ‘ä»¬éœ€è¦ä¿ç•™åŸå§‹ä¿¡æ¯ï¼Œå¹¶åŠ ä¸Š List Name\n",
    "        subset_detail = subset.copy()\n",
    "        subset_detail.insert(0, 'List_Name', list_name) # ç¬¬ä¸€åˆ—æ”¾ List åå­—\n",
    "        \n",
    "        # åªä¿ç•™å…³é”®åˆ— (ä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™é‡Œä¿ç•™çš„åˆ—)\n",
    "        # å‡è®¾åŸå§‹åˆ—åæ˜¯è¿™äº›ï¼Œå¦‚æœæ²¡æœ‰ä¼šå¿½ç•¥é”™è¯¯\n",
    "        keep_cols = ['List_Name', index_col, 'Tree', 'Points', 'Point', 'DetailedSplit']\n",
    "        existing_cols = [c for c in keep_cols if c in subset_detail.columns]\n",
    "        details_data.append(subset_detail[existing_cols])\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # ç”Ÿæˆ è¡¨ä¸€ (Summary) çš„æ•°æ®\n",
    "        # ------------------------------------------\n",
    "        # A. æå–è¿™ä¸€ç»„æ‰€æœ‰è§„åˆ™æ¶‰åŠçš„æŒ‡æ ‡\n",
    "        all_indicators_in_subset = [] # å­˜å‚¨è¿™ä¸€ç»„é‡Œæ¯ä¸€æ¡è§„åˆ™ç”¨åˆ°çš„æŒ‡æ ‡åˆ—è¡¨\n",
    "        \n",
    "        for idx, row in subset.iterrows():\n",
    "            # å‡è®¾è§„åˆ™æ–‡æœ¬åœ¨ 'DetailedSplit' åˆ—\n",
    "            rule_text = row.get('DetailedSplit', '')\n",
    "            feats = extract_indicators(rule_text)\n",
    "            all_indicators_in_subset.extend(feats)\n",
    "        \n",
    "        # B. è®¡ç®—é¢‘ç‡\n",
    "        total_rules = len(subset)\n",
    "        if total_rules > 0:\n",
    "            from collections import Counter\n",
    "            counts = Counter(all_indicators_in_subset)\n",
    "            \n",
    "            # Key Indicators: å‡ºç°é¢‘ç‡ >= 90%\n",
    "            key_indicators = [ind for ind, count in counts.items() if (count / total_rules) >= 0.9]\n",
    "            \n",
    "            # All Indicators: å‡ºç°è¿‡çš„æ‰€æœ‰æŒ‡æ ‡ (å»é‡å¹¶æ’åº)\n",
    "            unique_indicators = sorted(list(counts.keys()))\n",
    "        else:\n",
    "            key_indicators = []\n",
    "            unique_indicators = []\n",
    "            \n",
    "        # C. è·å–æ‰€æœ‰ Tree ID\n",
    "        if 'Tree' in subset.columns:\n",
    "            all_trees = sorted(subset['Tree'].unique().tolist())\n",
    "        else:\n",
    "            all_trees = []\n",
    "\n",
    "        # D. ç»„è£…æ±‡æ€»è¡Œ\n",
    "        summary_data.append({\n",
    "            'List_Name': list_name,\n",
    "            'Key_Indicators (>=90%)': \", \".join(key_indicators),\n",
    "            'All_Indicators': \", \".join(unique_indicators),\n",
    "            'All_Trees': \", \".join(map(str, all_trees)),\n",
    "            'All_Rules_Count': total_rules,\n",
    "            'All_Rule_Indices': \", \".join(indices) # å¦‚æœåˆ—è¡¨å¤ªé•¿ï¼ŒExcelé‡Œå¯èƒ½ä¼šæ˜¾ç¤ºä¸å…¨\n",
    "        })\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # åˆå¹¶ç»“æœ\n",
    "    # ------------------------------------------\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if details_data:\n",
    "        details_df = pd.concat(details_data, ignore_index=True)\n",
    "    else:\n",
    "        details_df = pd.DataFrame()\n",
    "        \n",
    "    return summary_df, details_df\n",
    "\n",
    "# ==========================================\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# ==========================================\n",
    "\n",
    "# 1. è¯»å–ä½ çš„åŸå§‹æ•°æ® (å‡è®¾å°±æ˜¯ä¹‹å‰çš„ tree.xlsx)\n",
    "# è¯·ç¡®ä¿ä½ çš„ Excel é‡Œæœ‰ 'DetailedSplit' å’Œ 'Index' (æˆ–ä½ è‡ªå·±å‘½åçš„ç´¢å¼•åˆ—)\n",
    "master_df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. å®šä¹‰ä½ çš„ Lists (è¿™å°±æ˜¯ä½ çš„è¾“å…¥)\n",
    "# ä½ å¯ä»¥æ‰‹åŠ¨å†™ï¼Œä¹Ÿå¯ä»¥ä»ä¹‹å‰çš„èšç±»ç»“æœä¸­è‡ªåŠ¨æå–\n",
    "my_rule_lists = {\n",
    "    \"Cluster_1\": [1, 3, 6, 10],   # ç¤ºä¾‹æ•°æ®ï¼Œè¯·æ›¿æ¢ä¸ºä½ çœŸå®çš„ Index\n",
    "    \"Cluster_2\": [2, 4, 5, 7, 8],\n",
    "    \"High_Risk_Group\": [201, 208, 209] \n",
    "}\n",
    "\n",
    "# 3. è¿è¡Œå‡½æ•°\n",
    "# æ³¨æ„ï¼šindex_col='Index' è¿™é‡Œçš„ 'Index' å¿…é¡»æ˜¯ä½  Excel ç¬¬ä¸€åˆ—çš„åˆ—å\n",
    "# å¦‚æœä½ çš„ç¬¬ä¸€åˆ—å« 'Rule_ID'ï¼Œå°±æ”¹æˆ index_col='Rule_ID'\n",
    "df_summary, df_details = generate_rule_tables(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 4. ä¿å­˜ç»“æœ\n",
    "with pd.ExcelWriter(\"Rule_Analysis_Report.xlsx\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name='Summary_Table_1', index=False)\n",
    "    df_details.to_excel(writer, sheet_name='Details_Table_2', index=False)\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ Rule_Analysis_Report.xlsx\")\n",
    "print(\"Sheet 1: æ±‡æ€»å¯¹æ¯” (Key Indicators, Trees...)\")\n",
    "print(\"Sheet 2: è§„åˆ™è¯¦æƒ… (List, Point, DetailedSplit...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b5d6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrity(rule_lists_dict, master_df, index_col='Index'):\n",
    "    \"\"\"\n",
    "    æ£€æŸ¥è§„åˆ™åˆ—è¡¨çš„å®Œæ•´æ€§ï¼šä¸é‡å¤ã€ä¸é—æ¼\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"æ­£åœ¨è¿›è¡Œå®Œæ•´æ€§æ£€æŸ¥ (MECE Check)...\")\n",
    "    \n",
    "    # 1. æ”¶é›†æ‰€æœ‰è¢«åˆ†é…çš„ Index\n",
    "    all_assigned = []\n",
    "    for indices in rule_lists_dict.values():\n",
    "        all_assigned.extend([str(i) for i in indices])\n",
    "    \n",
    "    # 2. è·å–åŸå§‹è¡¨ä¸­æ‰€æœ‰çš„ Index\n",
    "    all_master = set(master_df[index_col].astype(str).tolist())\n",
    "    assigned_set = set(all_assigned)\n",
    "    \n",
    "    # 3. æ£€æŸ¥é‡å¤ (Mutually Exclusive)\n",
    "    counts = Counter(all_assigned)\n",
    "    duplicates = [idx for idx, count in counts.items() if count > 1]\n",
    "    \n",
    "    # 4. æ£€æŸ¥é—æ¼ (Collectively Exhaustive)\n",
    "    missing = all_master - assigned_set\n",
    "    \n",
    "    # 5. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸å­˜åœ¨äºåŸå§‹è¡¨çš„ Index (å¹»è§‰æ£€æŸ¥)\n",
    "    extra = assigned_set - all_master\n",
    "\n",
    "    # è¾“å‡ºç»“æœ\n",
    "    passed = True\n",
    "    if duplicates:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šå‘ç° {len(duplicates)} ä¸ªé‡å¤ Index: {duplicates}\")\n",
    "        passed = False\n",
    "    if missing:\n",
    "        print(f\"âŒ é”™è¯¯ï¼šå‘ç° {len(missing)} ä¸ªé—æ¼ Index (æœªåŒ…å«åœ¨ä»»ä½• list ä¸­): {sorted(list(missing))}\")\n",
    "        passed = False\n",
    "    if extra:\n",
    "        print(f\"âš ï¸ è­¦å‘Šï¼šå‘ç° {len(extra)} ä¸ªä¸å­˜åœ¨äºåŸå§‹è¡¨çš„ Index: {extra}\")\n",
    "        passed = False\n",
    "        \n",
    "    if passed:\n",
    "        print(\"âœ… æ£€æŸ¥é€šè¿‡ï¼šæ‰€æœ‰è§„åˆ™ä¸é‡å¤ã€ä¸é—æ¼ï¼Œä¸”å…¨éƒ¨æœ‰æ•ˆã€‚\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48b862",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. è¯»å–åŸå§‹æ•°æ®\n",
    "master_df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. å®šä¹‰ä½ çš„ Lists (LLM ç»™ä½ çš„åˆ†ç±»ç»“æœ)\n",
    "my_rule_lists = {\n",
    "    \"Cluster_1\": [1, 3, 6, 10],   \n",
    "    \"Cluster_2\": [2, 4, 5, 7, 8],\n",
    "    \"High_Risk_Group\": [201, 208, 209] \n",
    "}\n",
    "\n",
    "# 3. ã€æ–°å¢æ­¥éª¤ã€‘å…ˆè¿è¡Œæ£€æŸ¥\n",
    "# å³ä½¿æ£€æŸ¥ä¸é€šè¿‡ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ç”Ÿæˆè¡¨æ ¼ï¼Œä½†ä½ ä¼šçœ‹åˆ°æŠ¥é”™æé†’\n",
    "check_integrity(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 4. è¿è¡Œç”Ÿæˆå‡½æ•°\n",
    "df_summary, df_details = generate_rule_tables(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 5. ä¿å­˜ç»“æœ\n",
    "with pd.ExcelWriter(\"Rule_Analysis_Report.xlsx\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name='Summary_Table_1', index=False)\n",
    "    df_details.to_excel(writer, sheet_name='Details_Table_2', index=False)\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02434a27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_single_cluster_prompt_data(cluster_label, summary_df, details_df):\n",
    "    \"\"\"\n",
    "    Generate prompt data for a SINGLE cluster, including ALL its rules.\n",
    "    \"\"\"\n",
    "    # 1. Get Summary Info\n",
    "    summary_row = summary_df[summary_df['List_Name'] == cluster_label].iloc[0]\n",
    "    avg_score = summary_row.get('Average_Point', 'N/A')\n",
    "    key_inds = summary_row['Key_Indicators (>=90%)']\n",
    "    all_inds = summary_row['All_Indicators']\n",
    "    count = summary_row['All_Rules_Count']\n",
    "    \n",
    "    # 2. Get ALL Rules Details\n",
    "    # Filter details_df for this specific cluster\n",
    "    subset = details_df[details_df['List_Name'] == cluster_label]\n",
    "    \n",
    "    # Format the data string\n",
    "    data_text = f\"=== TARGET CLUSTER: {cluster_label} ===\\n\"\n",
    "    data_text += f\"Statistics:\\n\"\n",
    "    data_text += f\"- Rule Count: {count}\\n\"\n",
    "    data_text += f\"- Average Risk Score: {avg_score}\\n\"\n",
    "    data_text += f\"- Key Indicators (freq>=90%): {key_inds}\\n\"\n",
    "    data_text += f\"- All Indicators Involved: {all_inds}\\n\\n\"\n",
    "    \n",
    "    data_text += f\"=== ALL INCLUDED RULES ({count} rules) ===\\n\"\n",
    "    # Iterate through all rules in this cluster\n",
    "    for idx, row in subset.iterrows():\n",
    "        # Clean up the DetailedSplit text\n",
    "        rule_content = str(row['DetailedSplit']).replace('\"', '').strip()\n",
    "        point = row.get('Point', row.get('Points', 'N/A'))\n",
    "        rule_idx = row.get('Index', 'N/A') # Assuming 'Index' column exists\n",
    "        \n",
    "        data_text += f\"[Rule Index: {rule_idx} | Score: {point}]\\n\"\n",
    "        data_text += f\"Condition: {rule_content}\\n\"\n",
    "        data_text += \"-\" * 20 + \"\\n\"\n",
    "        \n",
    "    return data_text\n",
    "\n",
    "# ==========================================\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# ==========================================\n",
    "# å‡è®¾æ‚¨æƒ³è·‘ Cluster 1\n",
    "target_cluster = \"Cluster_1\"  # è¯·ç¡®ä¿åå­—å’Œ summary_df é‡Œçš„ä¸€è‡´\n",
    "cluster_data_text = get_single_cluster_prompt_data(target_cluster, df_summary, df_details)\n",
    "\n",
    "print(cluster_data_text)\n",
    "# å¤åˆ¶æ‰“å°å‡ºæ¥çš„å†…å®¹ï¼Œç²˜è´´åˆ°ä¸‹é¢ Prompt çš„ [DATA_SECTION] å¤„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659ddbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Expert with 20 years of experience in financial crime investigation and model interpretation. You specialize in translating technical XGBoost rules into clear, actionable business risk scenarios.\n",
    "\n",
    "# Task\n",
    "I have performed Hierarchical Clustering on a set of fraud detection rules.\n",
    "I am providing you with **ALL the rules** belonging to a single specific cluster: **[CLUSTER_NAME]**.\n",
    "\n",
    "Your goal is to analyze these rules collectively to define the **Risk Persona** of this cluster. You need to explain *what* specific fraud scenario this group of rules is catching.\n",
    "\n",
    "# Context: Indicator Definitions\n",
    "Use this dictionary to interpret the business meaning of the rules:\n",
    "# Analysis Requirements\n",
    "Please analyze the provided data and output a report covering:\n",
    "\n",
    "1. **Scenario Name**: A professional, concise name for this risk cluster (e.g., \"xxxxx\").\n",
    "2. **Risk Narrative**: A detailed explanation of the fraud pattern.\n",
    "    - Connect the dots between the **Key Indicators**.\n",
    "    - Explain the business logic: \"Why do these specific indicators appear together?\"\n",
    "    - Example: \"This cluster targets companies in the xxxxx\"\n",
    "3. **Core Pattern**: The primary formula of this cluster (e.g., \"Pre-condition A + Over-leverage B + Asset Inflation C\").\n",
    "4. **Consistency Check**: Are there any rules in this list that seem to define a slightly different logic? If yes, briefly mention them as \"Variants\".\n",
    "\n",
    "# Input Data\n",
    "[DATA_SECTION_START]\n",
    "(Paste the Python output here)\n",
    "[DATA_SECTION_END]\n",
    "\n",
    "# Output Format\n",
    "Please provide the response in a structured Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e048b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Analyst. Your task is to interpret a specific fraud detection rule generated by an XGBoost model and translate it into a **single, concise business description**.\n",
    "\n",
    "# Context: Indicator Dictionary\n",
    "Use the following definitions to interpret the conditions:\n",
    "\n",
    "# Task Requirements\n",
    "1. **Analyze**: Look at the combination of indicators in the provided rule.\n",
    "2. **Synthesize**: Write **ONE sentence** describing what a company triggering this rule looks like from a business perspective.\n",
    "\n",
    "# Output Format\n",
    "Return **ONLY** the description sentence. Do not include \"Here is the description\" or quotes.\n",
    "\n",
    "# Input Rule\n",
    "[RULE_INFO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae87ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. å®šä¹‰ Prompt æ¨¡æ¿\n",
    "# ==========================================\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "# Role\n",
    "You are a Senior Fraud Risk Analyst. Your task is to interpret a specific fraud detection rule generated by an XGBoost model and translate it into a **single, concise business description**.\n",
    "\n",
    "# Context: Indicator Dictionary\n",
    "(Use the table provided above in your actual prompt string...)\n",
    "\n",
    "# Task Requirements\n",
    "1. Write **ONE sentence** describing the business logic of this rule.\n",
    "2. Focus on the *combination* of risks (e.g., \"\").\n",
    "3. Mention the specific risk type (e.g., \"\").\n",
    "\n",
    "# Output Format\n",
    "Return ONLY the description sentence.\n",
    "\n",
    "# Input Rule\n",
    "Index: {index}\n",
    "Score: {point}\n",
    "Conditions: {detailed_split}\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ¨¡æ‹Ÿ LLM è°ƒç”¨å‡½æ•° (è¯·æ›¿æ¢ä¸ºçœŸå® API)\n",
    "# ==========================================\n",
    "def call_llm_api(prompt_text):\n",
    "    \"\"\"\n",
    "    è¿™é‡Œæ˜¯æ¨¡æ‹Ÿå‡½æ•°ã€‚\n",
    "    å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·åœ¨è¿™é‡Œè°ƒç”¨ openai.ChatCompletion.create(...) \n",
    "    æˆ–è€…ä½ çš„å…¬å¸å†…éƒ¨ LLM æ¥å£ã€‚\n",
    "    \"\"\"\n",
    "    # æ¨¡æ‹Ÿè¿”å›ï¼šå‡è£… LLM å·²ç»ç†è§£äº†\n",
    "    # åœ¨çœŸå®ç¯å¢ƒï¼Œè¿™é‡Œåº”è¯¥æ˜¯: return response['choices'][0]['message']['content']\n",
    "    return \"This is a simulated description from LLM.\" \n",
    "\n",
    "# ==========================================\n",
    "# 3. ä¸»æµç¨‹\n",
    "# ==========================================\n",
    "def process_rules_one_by_one(input_file, output_file):\n",
    "    # è¯»å– Excel\n",
    "    df = pd.read_excel(input_file, sheet_name='Sheet1')\n",
    "    \n",
    "    # å‡†å¤‡ç»“æœåˆ—è¡¨\n",
    "    results = []\n",
    "    \n",
    "    print(f\"å¼€å§‹å¤„ç† {len(df)} æ¡è§„åˆ™...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        rule_idx = row.get('Index', idx) # è·å– Rule Index\n",
    "        point = row.get('Points', row.get('Point', 'N/A'))\n",
    "        split = str(row['DetailedSplit']).replace('\"', '').strip()\n",
    "        \n",
    "        # 1. æ„é€  Prompt\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æŠŠå®Œæ•´çš„å­—å…¸å­—ç¬¦ä¸²æ”¾è¿› template\n",
    "        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘åªå±•ç¤ºæ›¿æ¢å˜é‡çš„éƒ¨åˆ†\n",
    "        current_prompt = PROMPT_TEMPLATE.format(\n",
    "            index=rule_idx,\n",
    "            point=point,\n",
    "            detailed_split=split\n",
    "        )\n",
    "        \n",
    "        # 2. è°ƒç”¨ LLM (å¦‚æœæœ‰ä¸€æ¡æŠ¥é”™ï¼Œç”¨ try-except æ•è·ï¼Œä¸è¦ä¸­æ–­æ•´ä¸ªç¨‹åº)\n",
    "        try:\n",
    "            # === å¦‚æœä½ æœ‰ APIï¼Œå–æ¶ˆä¸‹é¢è¿™è¡Œçš„æ³¨é‡Š ===\n",
    "            # description = call_llm_api(current_prompt)\n",
    "            \n",
    "            # === å¦‚æœä½ æ˜¯æ‰‹åŠ¨åšï¼Œè¿™é‡Œåªæ˜¯ç”Ÿæˆ Prompt ç»™ä½ çœ‹ ===\n",
    "            description = \"PENDING_LLM_RESPONSE\" \n",
    "            # print(f\"Processing Rule {rule_idx}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Rule {rule_idx}: {e}\")\n",
    "            description = \"ERROR\"\n",
    "        \n",
    "        # 3. æ”¶é›†ç»“æœ\n",
    "        results.append({\n",
    "            'Rule_Index': rule_idx,\n",
    "            'Points': point,\n",
    "            'DetailedSplit': split,\n",
    "            'LLM_Description': description, # LLM ç”Ÿæˆçš„ä¸€å¥è¯æè¿°\n",
    "            'Full_Prompt': current_prompt   # (å¯é€‰) ä¿å­˜ç”Ÿæˆçš„ Prompt æ–¹ä¾¿è°ƒè¯•\n",
    "        })\n",
    "        \n",
    "        # (å¯é€‰) å¢åŠ å»¶æ—¶ï¼Œé˜²æ­¢ API Rate Limit\n",
    "        # time.sleep(0.5)\n",
    "\n",
    "    # 4. ä¿å­˜ç»“æœ\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(f\"å¤„ç†å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³ {output_file}\")\n",
    "\n",
    "# ==========================================\n",
    "# è¿è¡Œ\n",
    "# ==========================================\n",
    "# å‡è®¾ä½ çš„æ–‡ä»¶å« tree.xlsx\n",
    "# process_rules_one_by_one('tree.xlsx', 'Rule_Descriptions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c07fae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. é…ç½®åŒºåŸŸ\n",
    "# ==========================================\n",
    "INPUT_FILE = 'tree.xlsx'         # ä½ çš„åŸå§‹ Excel æ–‡ä»¶\n",
    "OUTPUT_DIR = 'rule_descriptions' # å­˜æ”¾ç»“æœ txt æ–‡ä»¶çš„æ–‡ä»¶å¤¹\n",
    "START_INDEX = 10                 # è‡ªå®šä¹‰å¼€å§‹çš„ Rule Index (åŒ…å«)\n",
    "END_INDEX = 20                   # è‡ªå®šä¹‰ç»“æŸçš„ Rule Index (åŒ…å«)\n",
    "\n",
    "# Prompt æ¨¡æ¿ (è¯·å¡«å…¥å®Œæ•´çš„ Indicator Dictionary)\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "# Role\n",
    "You are a Senior Fraud Risk Analyst. Your task is to interpret a specific fraud detection rule generated by an XGBoost model and translate it into a **single, concise business description**.\n",
    "\n",
    "# Context: Indicator Dictionary\n",
    "[æ­¤å¤„è¯·å¡«å…¥ä½ å®Œæ•´çš„ Indicator Table...]\n",
    "\n",
    "# Task Requirements\n",
    "1. Write **ONE sentence** describing the business logic of this rule.\n",
    "2. Focus on the *combination* of risks.\n",
    "3. Mention the specific risk type.\n",
    "\n",
    "# Output Format\n",
    "Return ONLY the description sentence.\n",
    "\n",
    "# Input Rule\n",
    "Index: {index}\n",
    "Score: {point}\n",
    "Conditions: {detailed_split}\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. æ¨¡æ‹Ÿ LLM è°ƒç”¨å‡½æ•°\n",
    "# ==========================================\n",
    "def call_llm_api(prompt_text):\n",
    "    \"\"\"\n",
    "    å®é™…ä½¿ç”¨æ—¶è¯·æ›¿æ¢ä¸ºçœŸå® API è°ƒç”¨\n",
    "    \"\"\"\n",
    "    # time.sleep(1) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ\n",
    "    return f\"Simulated description based on prompt length {len(prompt_text)}\"\n",
    "\n",
    "# ==========================================\n",
    "# 3. ä¸»å¤„ç†é€»è¾‘\n",
    "# ==========================================\n",
    "def process_specific_rules(input_file, output_dir, start_idx, end_idx):\n",
    "    # 1. ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"åˆ›å»ºç›®å½•: {output_dir}\")\n",
    "\n",
    "    # 2. è¯»å– Excel\n",
    "    try:\n",
    "        df = pd.read_excel(input_file, sheet_name='Sheet1')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}\")\n",
    "        return\n",
    "\n",
    "    print(f\"æ€»è§„åˆ™æ•°: {len(df)}ã€‚å‡†å¤‡å¤„ç† Index èŒƒå›´: {start_idx} åˆ° {end_idx}\")\n",
    "\n",
    "    # 3. éå†æ¯ä¸€è¡Œ\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # è·å– Rule Index (ç¡®ä¿è½¬ä¸ºæ•´æ•°è¿›è¡Œæ¯”è¾ƒ)\n",
    "        try:\n",
    "            rule_idx = int(row.get('Index'))\n",
    "        except (ValueError, TypeError):\n",
    "            # å¦‚æœ Index åˆ—ä¸æ˜¯æ•°å­—ï¼Œè·³è¿‡\n",
    "            continue\n",
    "\n",
    "        # --- ç­›é€‰é€»è¾‘: åªå¤„ç†æŒ‡å®šèŒƒå›´å†…çš„ Rules ---\n",
    "        if not (start_idx <= rule_idx <= end_idx):\n",
    "            continue\n",
    "\n",
    "        # --- æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ (å¢é‡æ›´æ–°æ ¸å¿ƒ) ---\n",
    "        file_name = f\"rule_{rule_idx}.txt\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"â­ï¸  [Rule {rule_idx}] æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚\")\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "\n",
    "        # --- å¼€å§‹å¤„ç† ---\n",
    "        print(f\"ğŸ”„ [Rule {rule_idx}] æ­£åœ¨ç”Ÿæˆæè¿°...\")\n",
    "        \n",
    "        point = row.get('Points', row.get('Point', 'N/A'))\n",
    "        split = str(row['DetailedSplit']).replace('\"', '').strip()\n",
    "\n",
    "        # æ„é€  Prompt\n",
    "        current_prompt = PROMPT_TEMPLATE.format(\n",
    "            index=rule_idx,\n",
    "            point=point,\n",
    "            detailed_split=split\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # è°ƒç”¨ LLM\n",
    "            description = call_llm_api(current_prompt)\n",
    "            \n",
    "            # ä¿å­˜åˆ° txt æ–‡ä»¶\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(description)\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ [Rule {rule_idx}] å¤„ç†å¤±è´¥: {e}\")\n",
    "\n",
    "    # 4. æ€»ç»“\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"æœ¬æ¬¡æ–°ç”Ÿæˆ: {processed_count} æ¡\")\n",
    "    print(f\"è·³è¿‡å·²å­˜åœ¨: {skipped_count} æ¡\")\n",
    "    print(f\"ç»“æœä¿å­˜åœ¨: {output_dir}/ ç›®å½•ä¸‹\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "# ==========================================\n",
    "# 4. è¿è¡Œ\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # ä½ å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹å‚æ•°ï¼Œæˆ–è€…ç›´æ¥ä¿®æ”¹é¡¶éƒ¨çš„é…ç½®å¸¸é‡\n",
    "    process_specific_rules(INPUT_FILE, OUTPUT_DIR, START_INDEX, END_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc85e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_results_to_excel(original_excel, txt_dir, output_excel):\n",
    "    # 1. è¯»å–åŸå§‹è¡¨æ ¼\n",
    "    df = pd.read_excel(original_excel, sheet_name='Sheet1')\n",
    "    \n",
    "    # 2. å‡†å¤‡ä¸€ä¸ªå­—å…¸æ¥å­˜ LLM çš„ç»“æœ\n",
    "    # key: rule_index, value: description\n",
    "    desc_map = {}\n",
    "    \n",
    "    # éå†æ–‡ä»¶å¤¹é‡Œçš„æ‰€æœ‰ txt\n",
    "    if os.path.exists(txt_dir):\n",
    "        for filename in os.listdir(txt_dir):\n",
    "            if filename.startswith(\"rule_\") and filename.endswith(\".txt\"):\n",
    "                # è§£ææ–‡ä»¶å rule_10.txt -> 10\n",
    "                try:\n",
    "                    idx = int(filename.replace(\"rule_\", \"\").replace(\".txt\", \"\"))\n",
    "                    with open(os.path.join(txt_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                        desc_map[idx] = f.read().strip()\n",
    "                except Exception as e:\n",
    "                    print(f\"è¯»å–æ–‡ä»¶ {filename} å‡ºé”™: {e}\")\n",
    "    \n",
    "    # 3. æŠŠç»“æœæ˜ å°„å› DataFrame\n",
    "    # ä½¿ç”¨ apply å‡½æ•°ï¼Œæ ¹æ® Index åˆ—å» desc_map é‡Œæ‰¾\n",
    "    df['LLM_Description'] = df['Index'].apply(lambda x: desc_map.get(x, \"Not Processed\"))\n",
    "    \n",
    "    # 4. ä¿å­˜\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"æ±‡æ€»å®Œæˆï¼å·²ä¿å­˜è‡³ {output_excel}\")\n",
    "\n",
    "# ä½¿ç”¨æ–¹æ³•\n",
    "# merge_results_to_excel('tree.xlsx', 'rule_descriptions', 'Final_Report.xlsx')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
