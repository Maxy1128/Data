{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5e49b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. 文本预处理函数：只保留 \"ind_xxx\" 这样的特征名\n",
    "# 例如将 \"ind_4e < 1 or missing\" 转化为 \"ind_4e\"\n",
    "def preprocess_rule(text):\n",
    "    import re\n",
    "    # 正则表达式匹配以 ind 开头的特征名\n",
    "    features = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(features)\n",
    "\n",
    "# 3. 应用预处理\n",
    "df['features_text'] = df['DetailedSplit'].apply(preprocess_rule)\n",
    "\n",
    "# 4. 构建矩阵 (One-Hot Encoding)\n",
    "# binary=True 表示我们只关心“有没有用到这个特征”，不关心用了几次\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df['features_text'])\n",
    "\n",
    "# 5. 转化为 DataFrame 查看 (这就是“规则-特征”矩阵)\n",
    "rule_feature_matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# 将原始的 Point（分数）拼回来，方便对照\n",
    "rule_feature_matrix['Score_Points'] = df['Point']\n",
    "\n",
    "print(rule_feature_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45524a65",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 0: 准备工作\n",
    "# ==========================================\n",
    "# 假设这是你所有的 17 个指标名称（请把这里换成你真实的列表）\n",
    "# 即使某些指标在 rules 里一次都没出现，它们也会作为全 0 列出现在结果中\n",
    "all_17_indicators = [\n",
    "    \"ind_4e\", \"ind_13b\", \"ind_3a_1\", \"ind_12f\", \"ind_2a_1\", \n",
    "    \"ind_13a_1\", \"ind_6\", \"ind_7\", \"ind_8\", \"ind_9\", \n",
    "    \"ind_10\", \"ind_11\", \"ind_14\", \"ind_15\", \"ind_16\", \n",
    "    \"ind_17\", \"ind_unused_example\" # 确保这里列出了全部 17 个\n",
    "]\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 1: 文本预处理 (保持 210 行不变)\n",
    "# ==========================================\n",
    "# 我们只提取特征名，不拆分行\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    # 提取所有 ind_ 开头的词\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "df['feature_text'] = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 2: 构建矩阵 (强制使用所有 17 个指标)\n",
    "# ==========================================\n",
    "# 关键点：使用 vocabulary 参数！\n",
    "# 这告诉程序：“只关注这17个词，其他的我不要；没出现的词也要给我留列位置。”\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=all_17_indicators)\n",
    "\n",
    "# 生成矩阵\n",
    "X = vectorizer.fit_transform(df['feature_text'])\n",
    "\n",
    "# 转化为 DataFrame，列名就是我们指定的顺序\n",
    "matrix_df = pd.DataFrame(X.toarray(), columns=all_17_indicators)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 3: 拼接结果 (加上 Rule 和 Point)\n",
    "# ==========================================\n",
    "# axis=1 表示左右横向拼接\n",
    "final_df = pd.concat([\n",
    "    df[['DetailedSplit', 'Point']],  # 第一列显示 Rule，第二列显示分数\n",
    "    matrix_df                        # 后面跟着 17 列指标矩阵\n",
    "], axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 4: 检查与保存\n",
    "# ==========================================\n",
    "print(f\"最终矩阵维度: {final_df.shape}\") \n",
    "# 预期输出: (210, 2 + 17) = (210, 19)\n",
    "\n",
    "# 预览一下\n",
    "print(final_df.head())\n",
    "\n",
    "# 保存结果\n",
    "final_df.to_excel(\"final_rule_matrix.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45672fc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 0: 准备工作\n",
    "# ==========================================\n",
    "# 您的 17 个指标名称 (请确保这里是全的)\n",
    "all_17_indicators = [\n",
    "    \"ind_4e\", \"ind_13b\", \"ind_3a_1\", \"ind_12f\", \"ind_2a_1\", \n",
    "    \"ind_13a_1\", \"ind_6\", \"ind_7\", \"ind_8\", \"ind_9\", \n",
    "    \"ind_10\", \"ind_11\", \"ind_14\", \"ind_15\", \"ind_16\", \n",
    "    \"ind_17\", \"ind_unused_example\" \n",
    "]\n",
    "\n",
    "# 读取数据\n",
    "# 假设您的第一列是 Index，我们用 index_col=0 读取它，或者把它读作普通列\n",
    "# 这里建议读作普通列，方便我们保存到结果里\n",
    "df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 【关键排查点 1】打印刚读进来时的行数\n",
    "print(f\"原始数据行数: {df.shape[0]}\") \n",
    "# 如果这里已经是 421，说明 Excel 文件本身就是脏的（之前存错了）\n",
    "\n",
    "# 假设第一列叫 'Index' (如果不是，请把 df.columns[0] 改成您的列名)\n",
    "index_col_name = df.columns[0] \n",
    "print(f\"我们将使用列 '{index_col_name}' 作为原始索引追踪\")\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 1: 文本预处理 (绝对不进行拆分)\n",
    "# ==========================================\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "df['feature_text'] = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 2: 构建矩阵\n",
    "# ==========================================\n",
    "vectorizer = CountVectorizer(binary=True, vocabulary=all_17_indicators)\n",
    "X = vectorizer.fit_transform(df['feature_text'])\n",
    "matrix_df = pd.DataFrame(X.toarray(), columns=all_17_indicators)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 3: 拼接结果 (带上原始 Index)\n",
    "# ==========================================\n",
    "final_df = pd.concat([\n",
    "    df[[index_col_name, 'DetailedSplit', 'Point']], # 把 Index 列放最前面\n",
    "    matrix_df\n",
    "], axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 步骤 4: 检查\n",
    "# ==========================================\n",
    "print(f\"最终矩阵维度: {final_df.shape}\")\n",
    "\n",
    "# 如果行数不对，我们通过 Index 看看是谁重复了\n",
    "if final_df.shape[0] > 210:\n",
    "    print(\"\\n警告：行数异常增加！正在查找重复的 Index...\")\n",
    "    duplicates = final_df[final_df.duplicated(subset=[index_col_name], keep=False)]\n",
    "    print(duplicates[[index_col_name, 'DetailedSplit']].head(10))\n",
    "    print(\"\\n如果看到上面的 Index 有重复，说明数据源里这些行被拆分了。\")\n",
    "\n",
    "# 保存\n",
    "final_df.to_excel(\"final_rule_matrix_with_index.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08670d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 读取矩阵\n",
    "df = pd.read_excel(\"final_rule_matrix.xlsx\")\n",
    "\n",
    "# 2. 提取只有 0/1 的特征部分 (假设从第3列开始是特征)\n",
    "# 您的列是: DetailedSplit, Point, ind_1, ind_2 ...\n",
    "feature_cols = df.columns[2:] \n",
    "X = df[feature_cols]\n",
    "\n",
    "# 3. 绘制热力图\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(X, cbar=False, cmap=\"Blues\")\n",
    "plt.title(\"Rule-Feature Heatmap (Dark Blue = Feature Used)\")\n",
    "plt.xlabel(\"Indicators\")\n",
    "plt.ylabel(\"Rule ID\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585af28",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 计算聚类连接矩阵 (使用 Jaccard 距离，适合 binary 数据)\n",
    "# method='average' 或 'complete' 通常效果较好\n",
    "Z = linkage(X, method='average', metric='jaccard')\n",
    "\n",
    "# 2. 绘制树状图 (帮助您决定切成几类)\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(Z)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Rule Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.axhline(y=0.7, c='r', ls='--', lw=2) # 画一条辅助线，看看切在这里会分出几类\n",
    "plt.show()\n",
    "\n",
    "# 3. 真正打标签 (假设我们根据树状图决定切成 8 类)\n",
    "# t=8 表示我们要 8 个簇\n",
    "labels = fcluster(Z, t=8, criterion='maxclust')\n",
    "df['Cluster_Label'] = labels\n",
    "\n",
    "print(df['Cluster_Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0568b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 对每个簇进行聚合分析\n",
    "cluster_profile = df.groupby('Cluster_Label')[feature_cols].mean()\n",
    "\n",
    "# 只要某个特征在该簇的出现率超过 80% (0.8)，我们就认为它是该簇的“核心特征”\n",
    "for cluster_id in cluster_profile.index:\n",
    "    row = cluster_profile.loc[cluster_id]\n",
    "    core_features = row[row > 0.8].index.tolist()\n",
    "    \n",
    "    # 计算该簇的平均风险分\n",
    "    avg_score = df[df['Cluster_Label'] == cluster_id]['Point'].mean()\n",
    "    \n",
    "    print(f\"=== Cluster {cluster_id} (风险分: {avg_score:.1f}) ===\")\n",
    "    print(f\"核心特征: {core_features}\")\n",
    "    print(f\"规则数量: {len(df[df['Cluster_Label'] == cluster_id])}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb36fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.stripplot(x=\"Cluster_Label\", y=\"Point\", data=df, jitter=0.2, size=5)\n",
    "plt.title(\"Score Distribution by Cluster\")\n",
    "plt.axhline(0, color='red', linestyle='--') # 0分线\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa9b276",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import numpy as np\n",
    "\n",
    "# ==========================================\n",
    "# 1. 读取数据 & 准备\n",
    "# ==========================================\n",
    "# 读取上一步生成的矩阵文件\n",
    "df = pd.read_excel(\"final_rule_matrix.xlsx\")\n",
    "\n",
    "# 提取特征列（假设从第3列 'ind_xxx' 开始是特征，前两列是 DetailedSplit 和 Point）\n",
    "# 请根据您的实际列名调整，确保 feature_cols 只包含 0/1 的指标列\n",
    "feature_cols = df.columns[2:] \n",
    "X = df[feature_cols]\n",
    "\n",
    "# ==========================================\n",
    "# 2. 执行聚类 (复用之前的逻辑)\n",
    "# ==========================================\n",
    "# 使用 Jaccard 距离进行层次聚类\n",
    "Z = linkage(X, method='average', metric='jaccard')\n",
    "\n",
    "# 设定聚类参数 (这里演示用 distance = 0.7 自动切分，您也可以改用 t=8, criterion='maxclust')\n",
    "threshold = 0.7\n",
    "labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# 将聚类结果打标到原始数据上\n",
    "df.insert(0, 'Cluster_Label', labels) # 把 Cluster_Label 插到第一列，显眼\n",
    "\n",
    "# 按 簇ID 和 分数(Point) 排序，方便阅读\n",
    "df = df.sort_values(by=['Cluster_Label', 'Point'], ascending=[True, False])\n",
    "\n",
    "# ==========================================\n",
    "# 3. 生成“簇画像”概览 (Summary)\n",
    "# ==========================================\n",
    "summary_list = []\n",
    "unique_labels = sorted(df['Cluster_Label'].unique())\n",
    "\n",
    "for label in unique_labels:\n",
    "    # 取出该簇的所有数据\n",
    "    sub_df = df[df['Cluster_Label'] == label]\n",
    "    \n",
    "    # 计算核心特征：在该簇中出现频率 > 80% 的特征\n",
    "    # mean() 会对 0/1 列求均值，即出现频率\n",
    "    feat_freq = sub_df[feature_cols].mean()\n",
    "    core_feats = feat_freq[feat_freq > 0.8].index.tolist()\n",
    "    \n",
    "    summary_list.append({\n",
    "        'Cluster_Label': label,\n",
    "        'Rule_Count': len(sub_df),            # 规则数量\n",
    "        'Avg_Point': sub_df['Point'].mean(),  # 平均分\n",
    "        'Min_Point': sub_df['Point'].min(),   # 最低分\n",
    "        'Max_Point': sub_df['Point'].max(),   # 最高分\n",
    "        'Core_Features': \", \".join(core_feats) # 核心特征列表\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# ==========================================\n",
    "# 4. 保存到 Excel (包含两个 Sheet)\n",
    "# ==========================================\n",
    "output_file = \"Fraud_Rules_Cluster_Report.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Sheet 1: 概览画像\n",
    "    summary_df.to_excel(writer, sheet_name='Cluster_Summary', index=False)\n",
    "    \n",
    "    # Sheet 2: 详细数据 (整行数据都在这里)\n",
    "    df.to_excel(writer, sheet_name='Cluster_Details', index=False)\n",
    "\n",
    "print(f\"报告已生成: {output_file}\")\n",
    "print(\"Sheet 'Cluster_Summary': 包含每个簇的画像统计\")\n",
    "print(\"Sheet 'Cluster_Details': 包含按簇归类的所有原始规则详情\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3fb82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "# ==========================================\n",
    "# 1. 读取原始数据 & 保留 Index\n",
    "# ==========================================\n",
    "# 读取 Excel\n",
    "# index_col=None 确保第一列被读作普通列，而不是索引（防止丢失）\n",
    "df = pd.read_excel(\"tree.xlsx\", sheet_name='Sheet1', index_col=None)\n",
    "\n",
    "# 获取第一列的列名（假设第一列就是您说的 Index）\n",
    "index_col_name = df.columns[0]\n",
    "print(f\"检测到原始 Index 列名为: {index_col_name}\")\n",
    "\n",
    "# 为了防止后续处理丢失，我们显式地把它重命名为 'Original_Index' (可选，也可保留原名)\n",
    "# 这里我们选择保留原名，但在输出时调整位置\n",
    "\n",
    "# ==========================================\n",
    "# 2. 准备聚类特征\n",
    "# ==========================================\n",
    "def extract_features(text):\n",
    "    import re\n",
    "    if pd.isna(text): return \"\"\n",
    "    feats = re.findall(r\"(ind[\\w_]+)\", str(text))\n",
    "    return \" \".join(feats)\n",
    "\n",
    "# 提取特征用于计算\n",
    "rule_text = df['DetailedSplit'].apply(extract_features)\n",
    "\n",
    "# 构建矩阵\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(rule_text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# ==========================================\n",
    "# 3. 执行聚类 (距离阈值 0.7)\n",
    "# ==========================================\n",
    "Z = linkage(X.toarray(), method='average', metric='jaccard')\n",
    "threshold = 0.7\n",
    "labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# ==========================================\n",
    "# 4. 整理结果表格\n",
    "# ==========================================\n",
    "# 插入 Cluster_Label 到第 1 列\n",
    "if 'Cluster_Label' in df.columns:\n",
    "    df.drop(columns=['Cluster_Label'], inplace=True)\n",
    "df.insert(0, 'Cluster_Label', labels)\n",
    "\n",
    "# === 关键步骤：调整列顺序 ===\n",
    "# 我们希望顺序是: Cluster_Label -> Original_Index -> Point -> 其他列\n",
    "# 先把 index 列移到第 2 列的位置 (紧跟 Cluster_Label)\n",
    "cols = list(df.columns)\n",
    "# 移除 index 列和 Cluster_Label (防止重复)\n",
    "cols.remove('Cluster_Label')\n",
    "cols.remove(index_col_name)\n",
    "# 重新组合：Cluster_Label, Index列, 其他列...\n",
    "new_order = ['Cluster_Label', index_col_name] + cols\n",
    "df = df[new_order]\n",
    "\n",
    "# 按 簇ID 正序，分数 倒序 排序\n",
    "df_sorted = df.sort_values(by=['Cluster_Label', 'Point'], ascending=[True, False])\n",
    "\n",
    "# ==========================================\n",
    "# 5. 生成画像概览\n",
    "# ==========================================\n",
    "X_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "X_df['Cluster_Label'] = labels\n",
    "\n",
    "summary_list = []\n",
    "unique_labels = sorted(df['Cluster_Label'].unique())\n",
    "\n",
    "for label in unique_labels:\n",
    "    sub_df = df[df['Cluster_Label'] == label]\n",
    "    \n",
    "    # 画像计算\n",
    "    sub_X = X_df[X_df['Cluster_Label'] == label]\n",
    "    feat_freq = sub_X.drop(columns=['Cluster_Label']).mean()\n",
    "    core_feats = feat_freq[feat_freq > 0.8].index.tolist()\n",
    "    \n",
    "    summary_list.append({\n",
    "        'Cluster_Label': label,\n",
    "        'Rule_Count': len(sub_df),\n",
    "        'Avg_Point': sub_df['Point'].mean(),\n",
    "        'Core_Features': \", \".join(core_feats),\n",
    "        # 顺便展示该簇里包含的几个 Index 样例，方便快速定位\n",
    "        'Sample_Indices': str(sub_df[index_col_name].head(5).tolist()) + \"...\" \n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# ==========================================\n",
    "# 6. 保存\n",
    "# ==========================================\n",
    "output_file = \"Fraud_Rules_Clustered_With_Index.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Cluster_Summary', index=False)\n",
    "    df_sorted.to_excel(writer, sheet_name='Cluster_Details', index=False)\n",
    "\n",
    "print(f\"文件已生成: {output_file}\")\n",
    "print(f\"Sheet 'Cluster_Details' 的第 1 列是 Cluster ID，第 2 列是原始 {index_col_name}。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973a946",
   "metadata": {},
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Expert with 20 years of experience in financial institutions. You specialize in interpreting machine learning model rules (specifically XGBoost) to uncover underlying fraud patterns and business anomalies.\n",
    "\n",
    "# Task\n",
    "I will provide 210 fraud detection rules extracted from an XGBoost model. Each rule consists of multiple logical conditions based on specific Indicators and an associated risk score (Points). \n",
    "Your task is to perform **Semantic Clustering** on these 210 rules based on their **actual business meanings**.\n",
    "\n",
    "# Context & Data Dictionary\n",
    "Before analyzing, please study the following data dictionary carefully to understand what each indicator represents:\n",
    "\n",
    "\n",
    "# Requirements\n",
    "1. **Focus on Logic, Not Thresholds**: Do not separate rules just because of minor threshold differences (e.g., < 1 vs < 1.01). Focus on the **core business intent**.\n",
    "2. **Identify Feature Combinations**: Pay attention to indicators that frequently appear together (e.g., High Inventory + Over-financing).\n",
    "3. **Cluster into Scenarios**: Group these 210 rules into **8 to 12 distinct Risk Scenarios**.\n",
    "\n",
    "# Output Format\n",
    "Please provide the results in the following format for each cluster:\n",
    "\n",
    "## Cluster X: [Professional Business Name, e.g., Inventory Inflation & Over-financing]\n",
    "- **Risk Logic**: [A concise summary of the fraud/risk pattern. E.g., The entity maintains excessive inventory while simultaneously over-borrowing, suggesting potential loan fraud or asset overstatement.]\n",
    "- **Core Indicators**: [The primary 2-3 indicators defining this cluster.]\n",
    "- **Severity**: [High/Medium/Low based on the average Points.]\n",
    "- **Included Rule Indices**: [List all Rule Indices belonging to this group, e.g., 201, 208, 209...]\n",
    "\n",
    "# Data\n",
    "Below are the 210 rules:\n",
    "[Paste your CSV data here, including Index, Points, and DetailedSplit columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aff66e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_indicators(text):\n",
    "    \"\"\"从 DetailedSplit 文本中提取指标名称 (ind_xxx)\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    # 正则表达式匹配 ind 开头的指标\n",
    "    # \\w+ 匹配字母、数字、下划线\n",
    "    return list(set(re.findall(r\"(ind[\\w_]+)\", str(text))))\n",
    "\n",
    "def generate_rule_tables(rule_lists_dict, master_df, index_col='Index'):\n",
    "    \"\"\"\n",
    "    生成规则汇总表(Table 1)和详情表(Table 2)\n",
    "    \n",
    "    Args:\n",
    "        rule_lists_dict (dict): { 'List Name': [rule_index_1, rule_index_2, ...] }\n",
    "        master_df (pd.DataFrame): 包含所有规则的原始数据表\n",
    "        index_col (str): 原始数据中代表 Rule Index 的列名\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (summary_df, details_df)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ------------------------------------------\n",
    "    # 准备工作\n",
    "    # ------------------------------------------\n",
    "    summary_data = []\n",
    "    details_data = []\n",
    "    \n",
    "    # 确保索引列是字符串或统一格式，方便匹配\n",
    "    master_df[index_col] = master_df[index_col].astype(str)\n",
    "    \n",
    "    # 遍历每一个 List\n",
    "    for list_name, indices in rule_lists_dict.items():\n",
    "        # 统一转为字符串列表\n",
    "        indices = [str(i) for i in indices]\n",
    "        \n",
    "        # 1. 提取该 List 对应的所有规则子集\n",
    "        # 使用 isin 快速筛选\n",
    "        subset = master_df[master_df[index_col].isin(indices)].copy()\n",
    "        \n",
    "        if subset.empty:\n",
    "            print(f\"Warning: List '{list_name}' 里的 Index 在原始表中都找不到，跳过。\")\n",
    "            continue\n",
    "            \n",
    "        # ------------------------------------------\n",
    "        # 生成 表二 (Details) 的数据\n",
    "        # ------------------------------------------\n",
    "        # 我们需要保留原始信息，并加上 List Name\n",
    "        subset_detail = subset.copy()\n",
    "        subset_detail.insert(0, 'List_Name', list_name) # 第一列放 List 名字\n",
    "        \n",
    "        # 只保留关键列 (你可以根据需要调整这里保留的列)\n",
    "        # 假设原始列名是这些，如果没有会忽略错误\n",
    "        keep_cols = ['List_Name', index_col, 'Tree', 'Points', 'Point', 'DetailedSplit']\n",
    "        existing_cols = [c for c in keep_cols if c in subset_detail.columns]\n",
    "        details_data.append(subset_detail[existing_cols])\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # 生成 表一 (Summary) 的数据\n",
    "        # ------------------------------------------\n",
    "        # A. 提取这一组所有规则涉及的指标\n",
    "        all_indicators_in_subset = [] # 存储这一组里每一条规则用到的指标列表\n",
    "        \n",
    "        for idx, row in subset.iterrows():\n",
    "            # 假设规则文本在 'DetailedSplit' 列\n",
    "            rule_text = row.get('DetailedSplit', '')\n",
    "            feats = extract_indicators(rule_text)\n",
    "            all_indicators_in_subset.extend(feats)\n",
    "        \n",
    "        # B. 计算频率\n",
    "        total_rules = len(subset)\n",
    "        if total_rules > 0:\n",
    "            from collections import Counter\n",
    "            counts = Counter(all_indicators_in_subset)\n",
    "            \n",
    "            # Key Indicators: 出现频率 >= 90%\n",
    "            key_indicators = [ind for ind, count in counts.items() if (count / total_rules) >= 0.9]\n",
    "            \n",
    "            # All Indicators: 出现过的所有指标 (去重并排序)\n",
    "            unique_indicators = sorted(list(counts.keys()))\n",
    "        else:\n",
    "            key_indicators = []\n",
    "            unique_indicators = []\n",
    "            \n",
    "        # C. 获取所有 Tree ID\n",
    "        if 'Tree' in subset.columns:\n",
    "            all_trees = sorted(subset['Tree'].unique().tolist())\n",
    "        else:\n",
    "            all_trees = []\n",
    "\n",
    "        # D. 组装汇总行\n",
    "        summary_data.append({\n",
    "            'List_Name': list_name,\n",
    "            'Key_Indicators (>=90%)': \", \".join(key_indicators),\n",
    "            'All_Indicators': \", \".join(unique_indicators),\n",
    "            'All_Trees': \", \".join(map(str, all_trees)),\n",
    "            'All_Rules_Count': total_rules,\n",
    "            'All_Rule_Indices': \", \".join(indices) # 如果列表太长，Excel里可能会显示不全\n",
    "        })\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # 合并结果\n",
    "    # ------------------------------------------\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    if details_data:\n",
    "        details_df = pd.concat(details_data, ignore_index=True)\n",
    "    else:\n",
    "        details_df = pd.DataFrame()\n",
    "        \n",
    "    return summary_df, details_df\n",
    "\n",
    "# ==========================================\n",
    "# 使用示例\n",
    "# ==========================================\n",
    "\n",
    "# 1. 读取你的原始数据 (假设就是之前的 tree.xlsx)\n",
    "# 请确保你的 Excel 里有 'DetailedSplit' 和 'Index' (或你自己命名的索引列)\n",
    "master_df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. 定义你的 Lists (这就是你的输入)\n",
    "# 你可以手动写，也可以从之前的聚类结果中自动提取\n",
    "my_rule_lists = {\n",
    "    \"Cluster_1\": [1, 3, 6, 10],   # 示例数据，请替换为你真实的 Index\n",
    "    \"Cluster_2\": [2, 4, 5, 7, 8],\n",
    "    \"High_Risk_Group\": [201, 208, 209] \n",
    "}\n",
    "\n",
    "# 3. 运行函数\n",
    "# 注意：index_col='Index' 这里的 'Index' 必须是你 Excel 第一列的列名\n",
    "# 如果你的第一列叫 'Rule_ID'，就改成 index_col='Rule_ID'\n",
    "df_summary, df_details = generate_rule_tables(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 4. 保存结果\n",
    "with pd.ExcelWriter(\"Rule_Analysis_Report.xlsx\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name='Summary_Table_1', index=False)\n",
    "    df_details.to_excel(writer, sheet_name='Details_Table_2', index=False)\n",
    "\n",
    "print(\"处理完成！结果已保存至 Rule_Analysis_Report.xlsx\")\n",
    "print(\"Sheet 1: 汇总对比 (Key Indicators, Trees...)\")\n",
    "print(\"Sheet 2: 规则详情 (List, Point, DetailedSplit...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b5d6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_integrity(rule_lists_dict, master_df, index_col='Index'):\n",
    "    \"\"\"\n",
    "    检查规则列表的完整性：不重复、不遗漏\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(\"正在进行完整性检查 (MECE Check)...\")\n",
    "    \n",
    "    # 1. 收集所有被分配的 Index\n",
    "    all_assigned = []\n",
    "    for indices in rule_lists_dict.values():\n",
    "        all_assigned.extend([str(i) for i in indices])\n",
    "    \n",
    "    # 2. 获取原始表中所有的 Index\n",
    "    all_master = set(master_df[index_col].astype(str).tolist())\n",
    "    assigned_set = set(all_assigned)\n",
    "    \n",
    "    # 3. 检查重复 (Mutually Exclusive)\n",
    "    counts = Counter(all_assigned)\n",
    "    duplicates = [idx for idx, count in counts.items() if count > 1]\n",
    "    \n",
    "    # 4. 检查遗漏 (Collectively Exhaustive)\n",
    "    missing = all_master - assigned_set\n",
    "    \n",
    "    # 5. 检查是否存在不存在于原始表的 Index (幻觉检查)\n",
    "    extra = assigned_set - all_master\n",
    "\n",
    "    # 输出结果\n",
    "    passed = True\n",
    "    if duplicates:\n",
    "        print(f\"❌ 错误：发现 {len(duplicates)} 个重复 Index: {duplicates}\")\n",
    "        passed = False\n",
    "    if missing:\n",
    "        print(f\"❌ 错误：发现 {len(missing)} 个遗漏 Index (未包含在任何 list 中): {sorted(list(missing))}\")\n",
    "        passed = False\n",
    "    if extra:\n",
    "        print(f\"⚠️ 警告：发现 {len(extra)} 个不存在于原始表的 Index: {extra}\")\n",
    "        passed = False\n",
    "        \n",
    "    if passed:\n",
    "        print(\"✅ 检查通过：所有规则不重复、不遗漏，且全部有效。\")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "    return passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48b862",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 读取原始数据\n",
    "master_df = pd.read_excel('tree.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 2. 定义你的 Lists (LLM 给你的分类结果)\n",
    "my_rule_lists = {\n",
    "    \"Cluster_1\": [1, 3, 6, 10],   \n",
    "    \"Cluster_2\": [2, 4, 5, 7, 8],\n",
    "    \"High_Risk_Group\": [201, 208, 209] \n",
    "}\n",
    "\n",
    "# 3. 【新增步骤】先运行检查\n",
    "# 即使检查不通过，也可以继续生成表格，但你会看到报错提醒\n",
    "check_integrity(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 4. 运行生成函数\n",
    "df_summary, df_details = generate_rule_tables(my_rule_lists, master_df, index_col='Index')\n",
    "\n",
    "# 5. 保存结果\n",
    "with pd.ExcelWriter(\"Rule_Analysis_Report.xlsx\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name='Summary_Table_1', index=False)\n",
    "    df_details.to_excel(writer, sheet_name='Details_Table_2', index=False)\n",
    "\n",
    "print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02434a27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_single_cluster_prompt_data(cluster_label, summary_df, details_df):\n",
    "    \"\"\"\n",
    "    Generate prompt data for a SINGLE cluster, including ALL its rules.\n",
    "    \"\"\"\n",
    "    # 1. Get Summary Info\n",
    "    summary_row = summary_df[summary_df['List_Name'] == cluster_label].iloc[0]\n",
    "    avg_score = summary_row.get('Average_Point', 'N/A')\n",
    "    key_inds = summary_row['Key_Indicators (>=90%)']\n",
    "    all_inds = summary_row['All_Indicators']\n",
    "    count = summary_row['All_Rules_Count']\n",
    "    \n",
    "    # 2. Get ALL Rules Details\n",
    "    # Filter details_df for this specific cluster\n",
    "    subset = details_df[details_df['List_Name'] == cluster_label]\n",
    "    \n",
    "    # Format the data string\n",
    "    data_text = f\"=== TARGET CLUSTER: {cluster_label} ===\\n\"\n",
    "    data_text += f\"Statistics:\\n\"\n",
    "    data_text += f\"- Rule Count: {count}\\n\"\n",
    "    data_text += f\"- Average Risk Score: {avg_score}\\n\"\n",
    "    data_text += f\"- Key Indicators (freq>=90%): {key_inds}\\n\"\n",
    "    data_text += f\"- All Indicators Involved: {all_inds}\\n\\n\"\n",
    "    \n",
    "    data_text += f\"=== ALL INCLUDED RULES ({count} rules) ===\\n\"\n",
    "    # Iterate through all rules in this cluster\n",
    "    for idx, row in subset.iterrows():\n",
    "        # Clean up the DetailedSplit text\n",
    "        rule_content = str(row['DetailedSplit']).replace('\"', '').strip()\n",
    "        point = row.get('Point', row.get('Points', 'N/A'))\n",
    "        rule_idx = row.get('Index', 'N/A') # Assuming 'Index' column exists\n",
    "        \n",
    "        data_text += f\"[Rule Index: {rule_idx} | Score: {point}]\\n\"\n",
    "        data_text += f\"Condition: {rule_content}\\n\"\n",
    "        data_text += \"-\" * 20 + \"\\n\"\n",
    "        \n",
    "    return data_text\n",
    "\n",
    "# ==========================================\n",
    "# 使用示例\n",
    "# ==========================================\n",
    "# 假设您想跑 Cluster 1\n",
    "target_cluster = \"Cluster_1\"  # 请确保名字和 summary_df 里的一致\n",
    "cluster_data_text = get_single_cluster_prompt_data(target_cluster, df_summary, df_details)\n",
    "\n",
    "print(cluster_data_text)\n",
    "# 复制打印出来的内容，粘贴到下面 Prompt 的 [DATA_SECTION] 处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659ddbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Expert with 20 years of experience in financial crime investigation and model interpretation. You specialize in translating technical XGBoost rules into clear, actionable business risk scenarios.\n",
    "\n",
    "# Task\n",
    "I have performed Hierarchical Clustering on a set of fraud detection rules.\n",
    "I am providing you with **ALL the rules** belonging to a single specific cluster: **[CLUSTER_NAME]**.\n",
    "\n",
    "Your goal is to analyze these rules collectively to define the **Risk Persona** of this cluster. You need to explain *what* specific fraud scenario this group of rules is catching.\n",
    "\n",
    "# Context: Indicator Definitions\n",
    "Use this dictionary to interpret the business meaning of the rules:\n",
    "# Analysis Requirements\n",
    "Please analyze the provided data and output a report covering:\n",
    "\n",
    "1. **Scenario Name**: A professional, concise name for this risk cluster (e.g., \"xxxxx\").\n",
    "2. **Risk Narrative**: A detailed explanation of the fraud pattern.\n",
    "    - Connect the dots between the **Key Indicators**.\n",
    "    - Explain the business logic: \"Why do these specific indicators appear together?\"\n",
    "    - Example: \"This cluster targets companies in the xxxxx\"\n",
    "3. **Core Pattern**: The primary formula of this cluster (e.g., \"Pre-condition A + Over-leverage B + Asset Inflation C\").\n",
    "4. **Consistency Check**: Are there any rules in this list that seem to define a slightly different logic? If yes, briefly mention them as \"Variants\".\n",
    "\n",
    "# Input Data\n",
    "[DATA_SECTION_START]\n",
    "(Paste the Python output here)\n",
    "[DATA_SECTION_END]\n",
    "\n",
    "# Output Format\n",
    "Please provide the response in a structured Markdown format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e048b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Role\n",
    "You are a Senior Fraud Risk Analyst. Your task is to interpret a specific fraud detection rule generated by an XGBoost model and translate it into a **single, concise business description**.\n",
    "\n",
    "# Context: Indicator Dictionary\n",
    "Use the following definitions to interpret the conditions:\n",
    "\n",
    "# Task Requirements\n",
    "1. **Analyze**: Look at the combination of indicators in the provided rule.\n",
    "2. **Synthesize**: Write **ONE sentence** describing what a company triggering this rule looks like from a business perspective.\n",
    "\n",
    "# Output Format\n",
    "Return **ONLY** the description sentence. Do not include \"Here is the description\" or quotes.\n",
    "\n",
    "# Input Rule\n",
    "[RULE_INFO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae87ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# ==========================================\n",
    "# 1. 定义 Prompt 模板\n",
    "# ==========================================\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "# Role\n",
    "You are a Senior Fraud Risk Analyst. Your task is to interpret a specific fraud detection rule generated by an XGBoost model and translate it into a **single, concise business description**.\n",
    "\n",
    "# Context: Indicator Dictionary\n",
    "(Use the table provided above in your actual prompt string...)\n",
    "\n",
    "# Task Requirements\n",
    "1. Write **ONE sentence** describing the business logic of this rule.\n",
    "2. Focus on the *combination* of risks (e.g., \"\").\n",
    "3. Mention the specific risk type (e.g., \"\").\n",
    "\n",
    "# Output Format\n",
    "Return ONLY the description sentence.\n",
    "\n",
    "# Input Rule\n",
    "Index: {index}\n",
    "Score: {point}\n",
    "Conditions: {detailed_split}\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. 模拟 LLM 调用函数 (请替换为真实 API)\n",
    "# ==========================================\n",
    "def call_llm_api(prompt_text):\n",
    "    \"\"\"\n",
    "    这里是模拟函数。\n",
    "    实际使用时，请在这里调用 openai.ChatCompletion.create(...) \n",
    "    或者你的公司内部 LLM 接口。\n",
    "    \"\"\"\n",
    "    # 模拟返回：假装 LLM 已经理解了\n",
    "    # 在真实环境，这里应该是: return response['choices'][0]['message']['content']\n",
    "    return \"This is a simulated description from LLM.\" \n",
    "\n",
    "# ==========================================\n",
    "# 3. 主流程\n",
    "# ==========================================\n",
    "def process_rules_one_by_one(input_file, output_file):\n",
    "    # 读取 Excel\n",
    "    df = pd.read_excel(input_file, sheet_name='Sheet1')\n",
    "    \n",
    "    # 准备结果列表\n",
    "    results = []\n",
    "    \n",
    "    print(f\"开始处理 {len(df)} 条规则...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        rule_idx = row.get('Index', idx) # 获取 Rule Index\n",
    "        point = row.get('Points', row.get('Point', 'N/A'))\n",
    "        split = str(row['DetailedSplit']).replace('\"', '').strip()\n",
    "        \n",
    "        # 1. 构造 Prompt\n",
    "        # 注意：这里需要把完整的字典字符串放进 template\n",
    "        # 为了演示，我只展示替换变量的部分\n",
    "        current_prompt = PROMPT_TEMPLATE.format(\n",
    "            index=rule_idx,\n",
    "            point=point,\n",
    "            detailed_split=split\n",
    "        )\n",
    "        \n",
    "        # 2. 调用 LLM (如果有一条报错，用 try-except 捕获，不要中断整个程序)\n",
    "        try:\n",
    "            # === 如果你有 API，取消下面这行的注释 ===\n",
    "            # description = call_llm_api(current_prompt)\n",
    "            \n",
    "            # === 如果你是手动做，这里只是生成 Prompt 给你看 ===\n",
    "            description = \"PENDING_LLM_RESPONSE\" \n",
    "            # print(f\"Processing Rule {rule_idx}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing Rule {rule_idx}: {e}\")\n",
    "            description = \"ERROR\"\n",
    "        \n",
    "        # 3. 收集结果\n",
    "        results.append({\n",
    "            'Rule_Index': rule_idx,\n",
    "            'Points': point,\n",
    "            'DetailedSplit': split,\n",
    "            'LLM_Description': description, # LLM 生成的一句话描述\n",
    "            'Full_Prompt': current_prompt   # (可选) 保存生成的 Prompt 方便调试\n",
    "        })\n",
    "        \n",
    "        # (可选) 增加延时，防止 API Rate Limit\n",
    "        # time.sleep(0.5)\n",
    "\n",
    "    # 4. 保存结果\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_excel(output_file, index=False)\n",
    "    print(f\"处理完成！结果已保存至 {output_file}\")\n",
    "\n",
    "# ==========================================\n",
    "# 运行\n",
    "# ==========================================\n",
    "# 假设你的文件叫 tree.xlsx\n",
    "# process_rules_one_by_one('tree.xlsx', 'Rule_Descriptions.xlsx')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
