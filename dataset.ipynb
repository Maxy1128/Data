{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377afde0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=r'sampledata_2.csv', \n",
    "    groundtruth_folder=r'groundtruth',\n",
    "    output_file='dataset_selection_result.xlsx',\n",
    "    target_total=3\n",
    "):\n",
    "    # --- 1. Read and preprocess sampledata.csv ---\n",
    "    print(\"Reading the original data table...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # Assume that file_name in CSV is {name}.html, we need to extract {name}\n",
    "    # Using os.path.splitext can safely handle filenames containing '.' (if there are other dots besides the extension)\n",
    "    df['clean_name'] = df['file_name'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # Check for duplicate clean_name to prevent matching confusion\n",
    "    if df['clean_name'].duplicated().any():\n",
    "        print(\"Warning: There are duplicate base filenames in sampledata.csv, which may affect matching accuracy.\")\n",
    "\n",
    "    # --- 2. Read and preprocess the groundtruth folder ---\n",
    "    print(\"Scanning the Groundtruth folder...\")\n",
    "    if not os.path.exists(groundtruth_folder):\n",
    "        print(f\"Error: Folder not found {groundtruth_folder}\")\n",
    "        return\n",
    "\n",
    "    # Get all .zip files in the folder\n",
    "    existing_files = glob.glob(os.path.join(groundtruth_folder, '*.zip'))\n",
    "    # Extract base filenames {name}, be careful with path separators\n",
    "    existing_basenames = [os.path.splitext(os.path.basename(f))[0] for f in existing_files]\n",
    "    \n",
    "    print(f\"There are {len(existing_basenames)} files in Groundtruth.\")\n",
    "\n",
    "    # --- 3. Mark existing data ---\n",
    "    # Mark in the dataframe whether the row data already exists in groundtruth\n",
    "    df['in_groundtruth'] = df['clean_name'].isin(existing_basenames)\n",
    "    \n",
    "    # Check if there are groundtruth files not found in CSV (to prevent filename mismatch issues)\n",
    "    matched_count = df['in_groundtruth'].sum()\n",
    "    if matched_count < len(existing_basenames):\n",
    "        missing = set(existing_basenames) - set(df[df['in_groundtruth']]['clean_name'])\n",
    "        print(f\"Warning: {len(existing_basenames) - matched_count} files in Groundtruth were not found in CSV.\")\n",
    "        print(f\"Unmatched examples: {list(missing)[:5]}\")\n",
    "\n",
    "    # --- 4. Calculate distribution and target quotas ---\n",
    "    # Count the topic distribution ratio of the 3500 data points\n",
    "    total_count = len(df)\n",
    "    topic_dist = df['topic'].value_counts(normalize=True) # Get proportions\n",
    "    \n",
    "    # Initialize statistics results list\n",
    "    stats_list = []\n",
    "    files_to_add_indices = []\n",
    "\n",
    "    print(\"Calculating quotas for each Topic and filling data...\")\n",
    "    \n",
    "    # Iterate through each topic (total 24)\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        # 1. Calculate how many should theoretically be in the 400 data points for this topic (round to nearest)\n",
    "        target_count = int(round(target_total * ratio))\n",
    "        if target_count == 0: target_count = 1 # Ensure at least 1 per category to avoid loss of small categories\n",
    "        \n",
    "        # 2. Get all data rows for this topic\n",
    "        topic_rows = df[df['topic'] == topic]\n",
    "        \n",
    "        # 3. Count how many in this topic are already in groundtruth\n",
    "        current_existing = topic_rows[topic_rows['in_groundtruth'] == True]\n",
    "        current_count = len(current_existing)\n",
    "        \n",
    "        # 4. Calculate the gap\n",
    "        needed = target_count - current_count\n",
    "        \n",
    "        added_count = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            # Need to add data\n",
    "            # Randomly sample from data in this topic that are not in groundtruth\n",
    "            candidates = topic_rows[topic_rows['in_groundtruth'] == False]\n",
    "            \n",
    "            if len(candidates) >= needed:\n",
    "                # Enough candidates, random sample (set random_state for reproducibility)\n",
    "                sampled = candidates.sample(n=needed, random_state=42)\n",
    "                files_to_add_indices.extend(sampled.index.tolist())\n",
    "                added_count = needed\n",
    "            else:\n",
    "                # Not enough candidates (shouldn't happen theoretically unless 3500 data itself is insufficient), select all\n",
    "                files_to_add_indices.extend(candidates.index.tolist())\n",
    "                added_count = len(candidates)\n",
    "                print(f\"Note: Insufficient data for Topic '{topic}', unable to fully meet target quota.\")\n",
    "        \n",
    "        # Record statistics\n",
    "        stats_list.append({\n",
    "            'Topic': topic,\n",
    "            'Original_Ratio': f\"{ratio:.2%}\",\n",
    "            'Target_Count_Total': target_count,\n",
    "            'Existing_In_Groundtruth': current_count,\n",
    "            'To_Add': added_count,\n",
    "            'Final_Total': current_count + added_count,\n",
    "            'Status': 'Over Budget' if needed < 0 else 'Filled'\n",
    "        })\n",
    "\n",
    "    # --- 5. Generate result DataFrame ---\n",
    "    \n",
    "    # Sheet 1: Distribution of existing 100+ data points\n",
    "    df_existing = df[df['in_groundtruth'] == True][['file_name', 'topic', 'category', 'clean_name']]\n",
    "    \n",
    "    # Sheet 2: List of file_names to add\n",
    "    df_to_add = df.loc[files_to_add_indices][['file_name', 'topic', 'category']]\n",
    "    \n",
    "    # Sheet 3: Overall distribution statistics table\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    # Adjust column order for easy viewing\n",
    "    df_stats = df_stats[['Topic', 'Original_Ratio', 'Target_Count_Total', 'Existing_In_Groundtruth', 'To_Add', 'Final_Total', 'Status']]\n",
    "\n",
    "    # Can also generate Sheet 4: Complete list of final 400 data points\n",
    "    df_final_list = pd.concat([df_existing, df_to_add])\n",
    "\n",
    "    # --- 6. Write to Excel ---\n",
    "    print(f\"Writing results to {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_existing.to_excel(writer, sheet_name='Existing_Distribution', index=False)\n",
    "        df_to_add.to_excel(writer, sheet_name='Files_To_Add', index=False)\n",
    "        df_stats.to_excel(writer, sheet_name='Distribution_Summary', index=False)\n",
    "        df_final_list.to_excel(writer, sheet_name='Final_Full_List', index=False)\n",
    "\n",
    "    print(\"Task completed!\")\n",
    "    print(f\"Total existing data: {len(df_existing)}\")\n",
    "    print(f\"Suggested data to add: {len(df_to_add)}\")\n",
    "    print(f\"Expected final total: {len(df_final_list)}\")\n",
    "\n",
    "# --- Execute function ---\n",
    "# Ensure sampledata.csv and groundtruth folder are in the current directory\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8cf983",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=\n",
    "    groundtruth_zip_path=\n",
    "    output_file='dataset_selection_result_checked.xlsx',\n",
    "    target_total=400\n",
    "):\n",
    "    # --- 1. 读取 CSV ---\n",
    "    print(\"Step 1: 读取 CSV 数据...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 找不到文件 {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # 提取 clean_name\n",
    "    df['clean_name'] = df['file_name'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    # 获取 CSV 中的所有唯一文件名集合\n",
    "    csv_filenames_set = set(df['clean_name'])\n",
    "    \n",
    "    # --- 2. 扫描 Zip ---\n",
    "    print(\"\\nStep 2: 扫描 Groundtruth Zip 包...\")\n",
    "    zip_filenames_set = set()\n",
    "    if os.path.exists(groundtruth_zip_path):\n",
    "        try:\n",
    "            with zipfile.ZipFile(groundtruth_zip_path, 'r') as z:\n",
    "                for f in z.namelist():\n",
    "                    if f.endswith('/') or '__MACOSX' in f: continue\n",
    "                    filename = os.path.basename(f)\n",
    "                    if not filename: continue\n",
    "                    zip_filenames_set.add(os.path.splitext(filename)[0])\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Error: Zip 文件损坏\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Warning: Zip文件不存在\")\n",
    "        return\n",
    "\n",
    "    print(f\"Groundtruth Zip 中包含 {len(zip_filenames_set)} 个唯一文件。\")\n",
    "\n",
    "    # --- 2.5 【诊断】检查 Zip 中有多少文件未在 CSV 中找到 ---\n",
    "    # 计算差集：在 Zip 中但不在 CSV 中的文件\n",
    "    unmatched_files = zip_filenames_set - csv_filenames_set\n",
    "    matched_files = zip_filenames_set.intersection(csv_filenames_set)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"【诊断结果】\")\n",
    "    print(f\"Zip 文件总数: {len(zip_filenames_set)}\")\n",
    "    print(f\"能与 CSV 匹配的文件数: {len(matched_files)} (这些会被优先选中)\")\n",
    "    print(f\"Zip 中有但 CSV 中没有的文件数: {len(unmatched_files)} (这些会被忽略)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 3. 数据清洗 (同名不同Topic不丢弃) ---\n",
    "    print(\"\\nStep 3: 计算分布...\")\n",
    "    df_clean = df.drop_duplicates(subset=['clean_name', 'topic']).copy()\n",
    "    \n",
    "    # 标记：只有在 Zip 里 且 在 CSV 里都能找到的，才算 in_groundtruth\n",
    "    df_clean['in_groundtruth'] = df_clean['clean_name'].isin(matched_files)\n",
    "    \n",
    "    topic_dist = df_clean['topic'].value_counts(normalize=True)\n",
    "    selected_filenames_set = set()\n",
    "    stats_list = []\n",
    "\n",
    "    # --- 4. 第一轮抽样 ---\n",
    "    print(\"\\nStep 4: 分层抽样 (优先复用)...\")\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        quota = int(round(target_total * ratio))\n",
    "        if quota == 0: quota = 1\n",
    "        \n",
    "        candidates = df_clean[df_clean['topic'] == topic]\n",
    "        covered_by_existing = candidates[candidates['clean_name'].isin(selected_filenames_set)]\n",
    "        count_covered = len(covered_by_existing)\n",
    "        \n",
    "        needed = quota - count_covered\n",
    "        added = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            pool = candidates[~candidates['clean_name'].isin(selected_filenames_set)]\n",
    "            pool_priority = pool[pool['in_groundtruth'] == True]\n",
    "            pool_normal = pool[pool['in_groundtruth'] == False]\n",
    "            \n",
    "            # 优先拿能匹配上的 Zip 文件\n",
    "            take_pri = min(len(pool_priority), needed)\n",
    "            if take_pri > 0:\n",
    "                picked = pool_priority.sample(n=take_pri, random_state=42)\n",
    "                selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                needed -= take_pri\n",
    "                added += take_pri\n",
    "                \n",
    "            if needed > 0:\n",
    "                take_norm = min(len(pool_normal), needed)\n",
    "                if take_norm > 0:\n",
    "                    picked = pool_normal.sample(n=take_norm, random_state=42)\n",
    "                    selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                    needed -= take_norm\n",
    "                    added += take_norm\n",
    "        \n",
    "        stats_list.append({\n",
    "            'Topic': topic, \n",
    "            'Quota': quota, \n",
    "            'Total_Filled': count_covered + added\n",
    "        })\n",
    "\n",
    "    # --- 5. 第二轮补齐 ---\n",
    "    current_count = len(selected_filenames_set)\n",
    "    gap = target_total - current_count\n",
    "    \n",
    "    if gap > 0:\n",
    "        print(f\"Step 5: 补齐剩余 {gap} 个名额...\")\n",
    "        all_cands = df_clean.drop_duplicates(subset=['clean_name'])\n",
    "        remaining = all_cands[~all_cands['clean_name'].isin(selected_filenames_set)]\n",
    "        \n",
    "        # 即使补齐，也只能补那些 CSV 里有的\n",
    "        rem_priority = remaining[remaining['in_groundtruth'] == True]\n",
    "        rem_normal = remaining[remaining['in_groundtruth'] == False]\n",
    "        \n",
    "        take_pri = min(len(rem_priority), gap)\n",
    "        if take_pri > 0:\n",
    "            picked = rem_priority.sample(n=take_pri, random_state=42)\n",
    "            selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "            gap -= take_pri\n",
    "            \n",
    "        if gap > 0 and len(rem_normal) > 0:\n",
    "            picked = rem_normal.sample(n=min(len(rem_normal), gap), random_state=42)\n",
    "            selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "\n",
    "    # --- 6. 导出 ---\n",
    "    print(\"\\nStep 6: 导出结果...\")\n",
    "    df_final_rows = df_clean[df_clean['clean_name'].isin(selected_filenames_set)].copy()\n",
    "    df_export = df_final_rows.drop_duplicates(subset=['clean_name'], keep='first')\n",
    "    \n",
    "    df_exist = df_export[df_export['in_groundtruth'] == True]\n",
    "    df_new = df_export[df_export['in_groundtruth'] == False]\n",
    "    \n",
    "    # 创建一个 DataFrame 来展示未匹配的文件\n",
    "    df_unmatched = pd.DataFrame({'Files_In_Zip_But_Not_In_CSV': list(unmatched_files)})\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_exist.to_excel(writer, sheet_name='1_Existing_In_Zip', index=False)\n",
    "        df_new.to_excel(writer, sheet_name='2_Files_To_Add', index=False)\n",
    "        # 新增这个 Sheet 帮你查错\n",
    "        df_unmatched.to_excel(writer, sheet_name='Check_Unmatched_Files', index=False)\n",
    "        pd.DataFrame(stats_list).to_excel(writer, sheet_name='Stats', index=False)\n",
    "        df_export.to_excel(writer, sheet_name='Final_List', index=False)\n",
    "\n",
    "    print(f\"Done! 请查看 '{output_file}' 中的 'Check_Unmatched_Files' 工作表，\")\n",
    "    print(f\"那里列出了 {len(unmatched_files)} 个被忽略的文件名。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
