{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377afde0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=r'sampledata_2.csv', \n",
    "    groundtruth_folder=r'groundtruth',\n",
    "    output_file='dataset_selection_result.xlsx',\n",
    "    target_total=3\n",
    "):\n",
    "    # --- 1. Read and preprocess sampledata.csv ---\n",
    "    print(\"Reading the original data table...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # Assume that file_name in CSV is {name}.html, we need to extract {name}\n",
    "    # Using os.path.splitext can safely handle filenames containing '.' (if there are other dots besides the extension)\n",
    "    df['clean_name'] = df['file_name'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # Check for duplicate clean_name to prevent matching confusion\n",
    "    if df['clean_name'].duplicated().any():\n",
    "        print(\"Warning: There are duplicate base filenames in sampledata.csv, which may affect matching accuracy.\")\n",
    "\n",
    "    # --- 2. Read and preprocess the groundtruth folder ---\n",
    "    print(\"Scanning the Groundtruth folder...\")\n",
    "    if not os.path.exists(groundtruth_folder):\n",
    "        print(f\"Error: Folder not found {groundtruth_folder}\")\n",
    "        return\n",
    "\n",
    "    # Get all .zip files in the folder\n",
    "    existing_files = glob.glob(os.path.join(groundtruth_folder, '*.zip'))\n",
    "    # Extract base filenames {name}, be careful with path separators\n",
    "    existing_basenames = [os.path.splitext(os.path.basename(f))[0] for f in existing_files]\n",
    "    \n",
    "    print(f\"There are {len(existing_basenames)} files in Groundtruth.\")\n",
    "\n",
    "    # --- 3. Mark existing data ---\n",
    "    # Mark in the dataframe whether the row data already exists in groundtruth\n",
    "    df['in_groundtruth'] = df['clean_name'].isin(existing_basenames)\n",
    "    \n",
    "    # Check if there are groundtruth files not found in CSV (to prevent filename mismatch issues)\n",
    "    matched_count = df['in_groundtruth'].sum()\n",
    "    if matched_count < len(existing_basenames):\n",
    "        missing = set(existing_basenames) - set(df[df['in_groundtruth']]['clean_name'])\n",
    "        print(f\"Warning: {len(existing_basenames) - matched_count} files in Groundtruth were not found in CSV.\")\n",
    "        print(f\"Unmatched examples: {list(missing)[:5]}\")\n",
    "\n",
    "    # --- 4. Calculate distribution and target quotas ---\n",
    "    # Count the topic distribution ratio of the 3500 data points\n",
    "    total_count = len(df)\n",
    "    topic_dist = df['topic'].value_counts(normalize=True) # Get proportions\n",
    "    \n",
    "    # Initialize statistics results list\n",
    "    stats_list = []\n",
    "    files_to_add_indices = []\n",
    "\n",
    "    print(\"Calculating quotas for each Topic and filling data...\")\n",
    "    \n",
    "    # Iterate through each topic (total 24)\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        # 1. Calculate how many should theoretically be in the 400 data points for this topic (round to nearest)\n",
    "        target_count = int(round(target_total * ratio))\n",
    "        if target_count == 0: target_count = 1 # Ensure at least 1 per category to avoid loss of small categories\n",
    "        \n",
    "        # 2. Get all data rows for this topic\n",
    "        topic_rows = df[df['topic'] == topic]\n",
    "        \n",
    "        # 3. Count how many in this topic are already in groundtruth\n",
    "        current_existing = topic_rows[topic_rows['in_groundtruth'] == True]\n",
    "        current_count = len(current_existing)\n",
    "        \n",
    "        # 4. Calculate the gap\n",
    "        needed = target_count - current_count\n",
    "        \n",
    "        added_count = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            # Need to add data\n",
    "            # Randomly sample from data in this topic that are not in groundtruth\n",
    "            candidates = topic_rows[topic_rows['in_groundtruth'] == False]\n",
    "            \n",
    "            if len(candidates) >= needed:\n",
    "                # Enough candidates, random sample (set random_state for reproducibility)\n",
    "                sampled = candidates.sample(n=needed, random_state=42)\n",
    "                files_to_add_indices.extend(sampled.index.tolist())\n",
    "                added_count = needed\n",
    "            else:\n",
    "                # Not enough candidates (shouldn't happen theoretically unless 3500 data itself is insufficient), select all\n",
    "                files_to_add_indices.extend(candidates.index.tolist())\n",
    "                added_count = len(candidates)\n",
    "                print(f\"Note: Insufficient data for Topic '{topic}', unable to fully meet target quota.\")\n",
    "        \n",
    "        # Record statistics\n",
    "        stats_list.append({\n",
    "            'Topic': topic,\n",
    "            'Original_Ratio': f\"{ratio:.2%}\",\n",
    "            'Target_Count_Total': target_count,\n",
    "            'Existing_In_Groundtruth': current_count,\n",
    "            'To_Add': added_count,\n",
    "            'Final_Total': current_count + added_count,\n",
    "            'Status': 'Over Budget' if needed < 0 else 'Filled'\n",
    "        })\n",
    "\n",
    "    # --- 5. Generate result DataFrame ---\n",
    "    \n",
    "    # Sheet 1: Distribution of existing 100+ data points\n",
    "    df_existing = df[df['in_groundtruth'] == True][['file_name', 'topic', 'category', 'clean_name']]\n",
    "    \n",
    "    # Sheet 2: List of file_names to add\n",
    "    df_to_add = df.loc[files_to_add_indices][['file_name', 'topic', 'category']]\n",
    "    \n",
    "    # Sheet 3: Overall distribution statistics table\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    # Adjust column order for easy viewing\n",
    "    df_stats = df_stats[['Topic', 'Original_Ratio', 'Target_Count_Total', 'Existing_In_Groundtruth', 'To_Add', 'Final_Total', 'Status']]\n",
    "\n",
    "    # Can also generate Sheet 4: Complete list of final 400 data points\n",
    "    df_final_list = pd.concat([df_existing, df_to_add])\n",
    "\n",
    "    # --- 6. Write to Excel ---\n",
    "    print(f\"Writing results to {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_existing.to_excel(writer, sheet_name='Existing_Distribution', index=False)\n",
    "        df_to_add.to_excel(writer, sheet_name='Files_To_Add', index=False)\n",
    "        df_stats.to_excel(writer, sheet_name='Distribution_Summary', index=False)\n",
    "        df_final_list.to_excel(writer, sheet_name='Final_Full_List', index=False)\n",
    "\n",
    "    print(\"Task completed!\")\n",
    "    print(f\"Total existing data: {len(df_existing)}\")\n",
    "    print(f\"Suggested data to add: {len(df_to_add)}\")\n",
    "    print(f\"Expected final total: {len(df_final_list)}\")\n",
    "\n",
    "# --- Execute function ---\n",
    "# Ensure sampledata.csv and groundtruth folder are in the current directory\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54586e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=\n",
    "    # 【新增】第二个表格的路径\n",
    "    extra_xlsx_path=, \n",
    "    groundtruth_zip_path=,\n",
    "    output_file='dataset_selection_result_merged.xlsx',\n",
    "    target_total=400\n",
    "):\n",
    "    print(\"Step 1: 读取并合并两个数据源...\")\n",
    "    \n",
    "    # --- 1.1 读取 CSV ---\n",
    "    df_csv = pd.DataFrame()\n",
    "    try:\n",
    "        print(f\"  -> 读取 CSV: {os.path.basename(csv_path)}\")\n",
    "        df_csv = pd.read_csv(csv_path)\n",
    "        print(f\"     包含 {len(df_csv)} 行数据\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 找不到 CSV 文件 {csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: 读取 CSV 失败 - {e}\")\n",
    "\n",
    "    # --- 1.2 读取 XLSX ---\n",
    "    df_xlsx = pd.DataFrame()\n",
    "    if os.path.exists(extra_xlsx_path):\n",
    "        try:\n",
    "            print(f\"  -> 读取 Excel: {os.path.basename(extra_xlsx_path)}\")\n",
    "            # 默认读取第一个 Sheet，如果有多个 Sheet 需要指定 sheet_name\n",
    "            df_xlsx = pd.read_excel(extra_xlsx_path, engine='openpyxl')\n",
    "            print(f\"     包含 {len(df_xlsx)} 行数据\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: 读取 Excel 失败 - {e}\")\n",
    "    else:\n",
    "        print(f\"Warning: Excel 文件不存在 {extra_xlsx_path}，将只使用 CSV 数据。\")\n",
    "\n",
    "    # --- 1.3 检查列名并合并 ---\n",
    "    # 确保两个表都有 file_name 和 topic 列\n",
    "    required_cols = ['file_name', 'topic']\n",
    "    \n",
    "    # 简单的列名检查与标准化 (防止 Excel 里叫 'Filename' 而 CSV 叫 'file_name')\n",
    "    # 这里假设结构差不多，直接合并。如果报错，需要手动 print(df_xlsx.columns) 看看列名。\n",
    "    \n",
    "    df_list = []\n",
    "    if not df_csv.empty:\n",
    "        # 确保包含关键列\n",
    "        if set(required_cols).issubset(df_csv.columns):\n",
    "            df_list.append(df_csv)\n",
    "        else:\n",
    "            print(f\"Error: CSV 文件缺少关键列 {required_cols}\")\n",
    "\n",
    "    if not df_xlsx.empty:\n",
    "        if set(required_cols).issubset(df_xlsx.columns):\n",
    "            df_list.append(df_xlsx)\n",
    "        else:\n",
    "            print(f\"Error: Excel 文件缺少关键列 {required_cols}，当前列: {df_xlsx.columns.tolist()}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"Error: 没有有效的数据被加载，程序终止。\")\n",
    "        return\n",
    "\n",
    "    # 合并\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"  -> 合并完成，原始数据总行数: {len(df)}\")\n",
    "\n",
    "    # 提取 clean_name\n",
    "    df['clean_name'] = df['file_name'].astype(str).apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # 获取合并后的所有文件名集合\n",
    "    csv_filenames_set = set(df['clean_name'])\n",
    "    \n",
    "    # --- 2. 扫描 Zip ---\n",
    "    print(\"\\nStep 2: 扫描 Groundtruth Zip 包...\")\n",
    "    zip_filenames_set = set()\n",
    "    if os.path.exists(groundtruth_zip_path):\n",
    "        try:\n",
    "            with zipfile.ZipFile(groundtruth_zip_path, 'r') as z:\n",
    "                for f in z.namelist():\n",
    "                    if f.endswith('/') or '__MACOSX' in f: continue\n",
    "                    filename = os.path.basename(f)\n",
    "                    if not filename: continue\n",
    "                    zip_filenames_set.add(os.path.splitext(filename)[0])\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Error: Zip 文件损坏\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Warning: Zip文件不存在\")\n",
    "        return\n",
    "\n",
    "    print(f\"Groundtruth Zip 中包含 {len(zip_filenames_set)} 个唯一文件。\")\n",
    "\n",
    "    # --- 2.5 【诊断】再次检查匹配情况 ---\n",
    "    unmatched_files = zip_filenames_set - csv_filenames_set\n",
    "    matched_files = zip_filenames_set.intersection(csv_filenames_set)\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"【合并数据源后的诊断结果】\")\n",
    "    print(f\"Zip 文件总数: {len(zip_filenames_set)}\")\n",
    "    print(f\"能与数据表匹配的文件数: {len(matched_files)} (提升匹配率的关键)\")\n",
    "    print(f\"仍然无法找到来源的文件数: {len(unmatched_files)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- 3. 数据清洗 (同名不同Topic不丢弃) ---\n",
    "    print(\"\\nStep 3: 计算分布...\")\n",
    "    # 去重：确保 (file_name, topic) 组合唯一\n",
    "    df_clean = df.drop_duplicates(subset=['clean_name', 'topic']).copy()\n",
    "    \n",
    "    # 标记：是否在 Zip 中\n",
    "    df_clean['in_groundtruth'] = df_clean['clean_name'].isin(matched_files)\n",
    "    \n",
    "    topic_dist = df_clean['topic'].value_counts(normalize=True)\n",
    "    selected_filenames_set = set()\n",
    "    stats_list = []\n",
    "\n",
    "    # --- 4. 第一轮抽样 ---\n",
    "    print(\"\\nStep 4: 分层抽样 (优先复用)...\")\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        quota = int(round(target_total * ratio))\n",
    "        if quota == 0: quota = 1\n",
    "        \n",
    "        candidates = df_clean[df_clean['topic'] == topic]\n",
    "        covered_by_existing = candidates[candidates['clean_name'].isin(selected_filenames_set)]\n",
    "        count_covered = len(covered_by_existing)\n",
    "        \n",
    "        needed = quota - count_covered\n",
    "        added = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            pool = candidates[~candidates['clean_name'].isin(selected_filenames_set)]\n",
    "            pool_priority = pool[pool['in_groundtruth'] == True]\n",
    "            pool_normal = pool[pool['in_groundtruth'] == False]\n",
    "            \n",
    "            # 优先拿 Zip\n",
    "            take_pri = min(len(pool_priority), needed)\n",
    "            if take_pri > 0:\n",
    "                picked = pool_priority.sample(n=take_pri, random_state=42)\n",
    "                selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                needed -= take_pri\n",
    "                added += take_pri\n",
    "                \n",
    "            # 拿普通\n",
    "            if needed > 0:\n",
    "                take_norm = min(len(pool_normal), needed)\n",
    "                if take_norm > 0:\n",
    "                    picked = pool_normal.sample(n=take_norm, random_state=42)\n",
    "                    selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                    needed -= take_norm\n",
    "                    added += take_norm\n",
    "        \n",
    "        stats_list.append({\n",
    "            'Topic': topic, \n",
    "            'Quota': quota, \n",
    "            'Total_Filled': count_covered + added\n",
    "        })\n",
    "\n",
    "    # --- 5. 第二轮补齐 ---\n",
    "    current_count = len(selected_filenames_set)\n",
    "    gap = target_total - current_count\n",
    "    \n",
    "    if gap > 0:\n",
    "        print(f\"Step 5: 补齐剩余 {gap} 个名额...\")\n",
    "        all_cands = df_clean.drop_duplicates(subset=['clean_name'])\n",
    "        remaining = all_cands[~all_cands['clean_name'].isin(selected_filenames_set)]\n",
    "        \n",
    "        rem_priority = remaining[remaining['in_groundtruth'] == True]\n",
    "        rem_normal = remaining[remaining['in_groundtruth'] == False]\n",
    "        \n",
    "        take_pri = min(len(rem_priority), gap)\n",
    "        if take_pri > 0:\n",
    "            picked = rem_priority.sample(n=take_pri, random_state=42)\n",
    "            selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "            gap -= take_pri\n",
    "            \n",
    "        if gap > 0 and len(rem_normal) > 0:\n",
    "            picked = rem_normal.sample(n=min(len(rem_normal), gap), random_state=42)\n",
    "            selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "\n",
    "    # --- 6. 导出 ---\n",
    "    print(\"\\nStep 6: 导出结果...\")\n",
    "    df_final_rows = df_clean[df_clean['clean_name'].isin(selected_filenames_set)].copy()\n",
    "    df_export = df_final_rows.drop_duplicates(subset=['clean_name'], keep='first')\n",
    "    \n",
    "    df_exist = df_export[df_export['in_groundtruth'] == True]\n",
    "    df_new = df_export[df_export['in_groundtruth'] == False]\n",
    "    df_unmatched = pd.DataFrame({'Still_Unmatched_Files': list(unmatched_files)})\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_exist.to_excel(writer, sheet_name='1_Existing_In_Zip', index=False)\n",
    "        df_new.to_excel(writer, sheet_name='2_Files_To_Add', index=False)\n",
    "        df_unmatched.to_excel(writer, sheet_name='Check_Unmatched', index=False)\n",
    "        pd.DataFrame(stats_list).to_excel(writer, sheet_name='Stats', index=False)\n",
    "        df_export.to_excel(writer, sheet_name='Final_List', index=False)\n",
    "\n",
    "    print(f\"Done! 结果已保存至 {output_file}\")\n",
    "    print(f\"最终唯一文件数: {len(df_export)}\")\n",
    "    print(f\"  - 来自 Zip (现有): {len(df_exist)}\")\n",
    "    print(f\"  - 来自 数据表 (新增): {len(df_new)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a906b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStep 6: 导出结果...\")\n",
    "    \n",
    "    # df_final_rows 包含了所有选中文件的所有 Topic 记录 (行数 > 400)\n",
    "    # 这就是你想要的“包含同一个file不同topic”的完整表\n",
    "    df_final_rows = df_clean[df_clean['clean_name'].isin(selected_filenames_set)].copy()\n",
    "    \n",
    "    # df_export 是为了方便文件操作，强制去重到 400 行 (每个文件只显示一行)\n",
    "    df_export = df_final_rows.drop_duplicates(subset=['clean_name'], keep='first')\n",
    "    \n",
    "    # 拆分 Existing 和 New (基于去重后的列表，方便你点数)\n",
    "    df_exist = df_export[df_export['in_groundtruth'] == True]\n",
    "    df_new = df_export[df_export['in_groundtruth'] == False]\n",
    "    \n",
    "    # 未匹配文件诊断\n",
    "    df_unmatched = pd.DataFrame({'Still_Unmatched_Files': list(unmatched_files)})\n",
    "\n",
    "    print(f\"写入结果到 {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Sheet 1: 现有文件 (物理文件清单)\n",
    "        df_exist.to_excel(writer, sheet_name='1_Existing_In_Zip', index=False)\n",
    "        \n",
    "        # Sheet 2: 新增文件 (物理文件清单)\n",
    "        df_new.to_excel(writer, sheet_name='2_Files_To_Add', index=False)\n",
    "        \n",
    "        # Sheet 3: 统计信息\n",
    "        pd.DataFrame(stats_list).to_excel(writer, sheet_name='3_Stats', index=False)\n",
    "        \n",
    "        # Sheet 4: 最终物理文件清单 (400行，去重过)\n",
    "        df_export.to_excel(writer, sheet_name='4_Final_List_Unique', index=False)\n",
    "        \n",
    "        # 【新增】Sheet 5: 完整详情 (包含多Topic情况，行数 > 400)\n",
    "        # 这里可以看到同一个 file 对应的所有 topic\n",
    "        df_final_rows.sort_values(by=['clean_name', 'topic']).to_excel(writer, sheet_name='5_Full_Details_Multi_Topic', index=False)\n",
    "        \n",
    "        # Sheet 6: 错误诊断\n",
    "        df_unmatched.to_excel(writer, sheet_name='Check_Unmatched', index=False)\n",
    "\n",
    "    print(f\"Done! 结果已保存至 {output_file}\")\n",
    "    print(f\"最终唯一文件数: {len(df_export)} (见 Sheet 4)\")\n",
    "    print(f\"包含多Topic的详细记录数: {len(df_final_rows)} (见 Sheet 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26b52df",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nStep 6: 导出结果...\")\n",
    "    \n",
    "    # df_final_rows 包含了所有选中文件的所有 Topic 记录 (行数 > 400)\n",
    "    # 这就是你想要的“包含同一个file不同topic”的完整表\n",
    "    df_final_rows = df_clean[df_clean['clean_name'].isin(selected_filenames_set)].copy()\n",
    "    \n",
    "    # df_export 是为了方便文件操作，强制去重到 400 行 (每个文件只显示一行)\n",
    "    df_export = df_final_rows.drop_duplicates(subset=['clean_name'], keep='first')\n",
    "    \n",
    "    # 拆分 Existing 和 New (基于去重后的列表，方便你点数)\n",
    "    df_exist = df_export[df_export['in_groundtruth'] == True]\n",
    "    df_new = df_export[df_export['in_groundtruth'] == False]\n",
    "    \n",
    "    # 未匹配文件诊断\n",
    "    df_unmatched = pd.DataFrame({'Still_Unmatched_Files': list(unmatched_files)})\n",
    "\n",
    "    print(f\"写入结果到 {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        # Sheet 1: 现有文件 (物理文件清单)\n",
    "        df_exist.to_excel(writer, sheet_name='1_Existing_In_Zip', index=False)\n",
    "        \n",
    "        # Sheet 2: 新增文件 (物理文件清单)\n",
    "        df_new.to_excel(writer, sheet_name='2_Files_To_Add', index=False)\n",
    "        \n",
    "        # Sheet 3: 统计信息\n",
    "        pd.DataFrame(stats_list).to_excel(writer, sheet_name='3_Stats', index=False)\n",
    "        \n",
    "        # Sheet 4: 最终物理文件清单 (400行，去重过)\n",
    "        df_export.to_excel(writer, sheet_name='4_Final_List_Unique', index=False)\n",
    "        \n",
    "        # 【新增】Sheet 5: 完整详情 (包含多Topic情况，行数 > 400)\n",
    "        # 这里可以看到同一个 file 对应的所有 topic\n",
    "        df_final_rows.sort_values(by=['clean_name', 'topic']).to_excel(writer, sheet_name='5_Full_Details_Multi_Topic', index=False)\n",
    "        \n",
    "        # Sheet 6: 错误诊断\n",
    "        df_unmatched.to_excel(writer, sheet_name='Check_Unmatched', index=False)\n",
    "\n",
    "    print(f\"Done! 结果已保存至 {output_file}\")\n",
    "    print(f\"最终唯一文件数: {len(df_export)} (见 Sheet 4)\")\n",
    "    print(f\"包含多Topic的详细记录数: {len(df_final_rows)} (见 Sheet 5)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
