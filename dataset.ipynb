{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377afde0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=r'sampledata_2.csv', \n",
    "    groundtruth_folder=r'groundtruth',\n",
    "    output_file='dataset_selection_result.xlsx',\n",
    "    target_total=3\n",
    "):\n",
    "    # --- 1. Read and preprocess sampledata.csv ---\n",
    "    print(\"Reading the original data table...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # Assume that file_name in CSV is {name}.html, we need to extract {name}\n",
    "    # Using os.path.splitext can safely handle filenames containing '.' (if there are other dots besides the extension)\n",
    "    df['clean_name'] = df['file_name'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # Check for duplicate clean_name to prevent matching confusion\n",
    "    if df['clean_name'].duplicated().any():\n",
    "        print(\"Warning: There are duplicate base filenames in sampledata.csv, which may affect matching accuracy.\")\n",
    "\n",
    "    # --- 2. Read and preprocess the groundtruth folder ---\n",
    "    print(\"Scanning the Groundtruth folder...\")\n",
    "    if not os.path.exists(groundtruth_folder):\n",
    "        print(f\"Error: Folder not found {groundtruth_folder}\")\n",
    "        return\n",
    "\n",
    "    # Get all .zip files in the folder\n",
    "    existing_files = glob.glob(os.path.join(groundtruth_folder, '*.zip'))\n",
    "    # Extract base filenames {name}, be careful with path separators\n",
    "    existing_basenames = [os.path.splitext(os.path.basename(f))[0] for f in existing_files]\n",
    "    \n",
    "    print(f\"There are {len(existing_basenames)} files in Groundtruth.\")\n",
    "\n",
    "    # --- 3. Mark existing data ---\n",
    "    # Mark in the dataframe whether the row data already exists in groundtruth\n",
    "    df['in_groundtruth'] = df['clean_name'].isin(existing_basenames)\n",
    "    \n",
    "    # Check if there are groundtruth files not found in CSV (to prevent filename mismatch issues)\n",
    "    matched_count = df['in_groundtruth'].sum()\n",
    "    if matched_count < len(existing_basenames):\n",
    "        missing = set(existing_basenames) - set(df[df['in_groundtruth']]['clean_name'])\n",
    "        print(f\"Warning: {len(existing_basenames) - matched_count} files in Groundtruth were not found in CSV.\")\n",
    "        print(f\"Unmatched examples: {list(missing)[:5]}\")\n",
    "\n",
    "    # --- 4. Calculate distribution and target quotas ---\n",
    "    # Count the topic distribution ratio of the 3500 data points\n",
    "    total_count = len(df)\n",
    "    topic_dist = df['topic'].value_counts(normalize=True) # Get proportions\n",
    "    \n",
    "    # Initialize statistics results list\n",
    "    stats_list = []\n",
    "    files_to_add_indices = []\n",
    "\n",
    "    print(\"Calculating quotas for each Topic and filling data...\")\n",
    "    \n",
    "    # Iterate through each topic (total 24)\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        # 1. Calculate how many should theoretically be in the 400 data points for this topic (round to nearest)\n",
    "        target_count = int(round(target_total * ratio))\n",
    "        if target_count == 0: target_count = 1 # Ensure at least 1 per category to avoid loss of small categories\n",
    "        \n",
    "        # 2. Get all data rows for this topic\n",
    "        topic_rows = df[df['topic'] == topic]\n",
    "        \n",
    "        # 3. Count how many in this topic are already in groundtruth\n",
    "        current_existing = topic_rows[topic_rows['in_groundtruth'] == True]\n",
    "        current_count = len(current_existing)\n",
    "        \n",
    "        # 4. Calculate the gap\n",
    "        needed = target_count - current_count\n",
    "        \n",
    "        added_count = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            # Need to add data\n",
    "            # Randomly sample from data in this topic that are not in groundtruth\n",
    "            candidates = topic_rows[topic_rows['in_groundtruth'] == False]\n",
    "            \n",
    "            if len(candidates) >= needed:\n",
    "                # Enough candidates, random sample (set random_state for reproducibility)\n",
    "                sampled = candidates.sample(n=needed, random_state=42)\n",
    "                files_to_add_indices.extend(sampled.index.tolist())\n",
    "                added_count = needed\n",
    "            else:\n",
    "                # Not enough candidates (shouldn't happen theoretically unless 3500 data itself is insufficient), select all\n",
    "                files_to_add_indices.extend(candidates.index.tolist())\n",
    "                added_count = len(candidates)\n",
    "                print(f\"Note: Insufficient data for Topic '{topic}', unable to fully meet target quota.\")\n",
    "        \n",
    "        # Record statistics\n",
    "        stats_list.append({\n",
    "            'Topic': topic,\n",
    "            'Original_Ratio': f\"{ratio:.2%}\",\n",
    "            'Target_Count_Total': target_count,\n",
    "            'Existing_In_Groundtruth': current_count,\n",
    "            'To_Add': added_count,\n",
    "            'Final_Total': current_count + added_count,\n",
    "            'Status': 'Over Budget' if needed < 0 else 'Filled'\n",
    "        })\n",
    "\n",
    "    # --- 5. Generate result DataFrame ---\n",
    "    \n",
    "    # Sheet 1: Distribution of existing 100+ data points\n",
    "    df_existing = df[df['in_groundtruth'] == True][['file_name', 'topic', 'category', 'clean_name']]\n",
    "    \n",
    "    # Sheet 2: List of file_names to add\n",
    "    df_to_add = df.loc[files_to_add_indices][['file_name', 'topic', 'category']]\n",
    "    \n",
    "    # Sheet 3: Overall distribution statistics table\n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "    # Adjust column order for easy viewing\n",
    "    df_stats = df_stats[['Topic', 'Original_Ratio', 'Target_Count_Total', 'Existing_In_Groundtruth', 'To_Add', 'Final_Total', 'Status']]\n",
    "\n",
    "    # Can also generate Sheet 4: Complete list of final 400 data points\n",
    "    df_final_list = pd.concat([df_existing, df_to_add])\n",
    "\n",
    "    # --- 6. Write to Excel ---\n",
    "    print(f\"Writing results to {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_existing.to_excel(writer, sheet_name='Existing_Distribution', index=False)\n",
    "        df_to_add.to_excel(writer, sheet_name='Files_To_Add', index=False)\n",
    "        df_stats.to_excel(writer, sheet_name='Distribution_Summary', index=False)\n",
    "        df_final_list.to_excel(writer, sheet_name='Final_Full_List', index=False)\n",
    "\n",
    "    print(\"Task completed!\")\n",
    "    print(f\"Total existing data: {len(df_existing)}\")\n",
    "    print(f\"Suggested data to add: {len(df_to_add)}\")\n",
    "    print(f\"Expected final total: {len(df_final_list)}\")\n",
    "\n",
    "# --- Execute function ---\n",
    "# Ensure sampledata.csv and groundtruth folder are in the current directory\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4fe17e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import math\n",
    "\n",
    "def generate_stratified_dataset(\n",
    "    csv_path=r'C:\\Users\\MaXin\\Desktop\\HSBC\\GroundTruth_Dataset\\sampledata_2.csv', \n",
    "    groundtruth_zip_path=r'C:\\Users\\MaXin\\Desktop\\HSBC\\GroundTruth_Dataset\\groundtruth.zip',\n",
    "    output_file='dataset_selection_result_400_overlap.xlsx',\n",
    "    target_total_unique=400  # 目标：至少400个唯一文件\n",
    "):\n",
    "    # --- 1. 读取数据 ---\n",
    "    print(\"Step 1: 读取原始数据...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: 找不到文件 {csv_path}\")\n",
    "        return\n",
    "\n",
    "    # 提取 clean_name\n",
    "    df['clean_name'] = df['file_name'].apply(lambda x: os.path.splitext(x)[0])\n",
    "    \n",
    "    # --- 2. 扫描 Zip 包 ---\n",
    "    print(\"\\nStep 2: 扫描 Groundtruth Zip 包...\")\n",
    "    existing_basenames = set()\n",
    "    if os.path.exists(groundtruth_zip_path):\n",
    "        try:\n",
    "            with zipfile.ZipFile(groundtruth_zip_path, 'r') as z:\n",
    "                for f in z.namelist():\n",
    "                    if f.endswith('/') or '__MACOSX' in f: continue\n",
    "                    filename = os.path.basename(f)\n",
    "                    if not filename: continue\n",
    "                    existing_basenames.add(os.path.splitext(filename)[0])\n",
    "        except zipfile.BadZipFile:\n",
    "            print(\"Error: Zip 文件损坏\")\n",
    "            return\n",
    "    else:\n",
    "        print(\"Warning: Zip文件不存在，视为全量新增。\")\n",
    "\n",
    "    print(f\"Groundtruth Zip 中包含 {len(existing_basenames)} 个唯一文件。\")\n",
    "\n",
    "    # --- 3. 数据预处理 ---\n",
    "    print(\"\\nStep 3: 计算分布 (允许同名不同Topic)...\")\n",
    "    \n",
    "    # 1. 彻底去重脏数据：完全一样的行 (File+Topic相同) 删掉\n",
    "    df_clean = df.drop_duplicates(subset=['clean_name', 'topic']).copy()\n",
    "    \n",
    "    # 2. 标记是否在 Zip 中 (辅助列)\n",
    "    df_clean['in_groundtruth'] = df_clean['clean_name'].isin(existing_basenames)\n",
    "    \n",
    "    # 3. 计算 Topic 分布 (分母是 df_clean 的行数，反映真实 Topic 权重)\n",
    "    # 注意：target_total_unique 是唯一文件数，但这里的 quota 是“人次”\n",
    "    # 如果有很多多Topic文件，总“人次”可能会超过 400。\n",
    "    # 我们这里先按 400 为基数计算“最小配额”，不够再补。\n",
    "    topic_dist = df_clean['topic'].value_counts(normalize=True)\n",
    "    \n",
    "    # 用于记录最终选中的唯一文件名\n",
    "    selected_filenames_set = set()\n",
    "    \n",
    "    stats_list = []\n",
    "\n",
    "    # --- 4. 第一轮：满足 Topic 配额 (允许复用) ---\n",
    "    print(\"\\nStep 4: 第一轮抽样 - 满足各 Topic 配额 (允许复用)...\")\n",
    "    \n",
    "    # 为了让复用最大化，可以考虑先处理已经在 Zip 里的文件\n",
    "    # 这里我们按 Topic 遍历\n",
    "    for topic, ratio in topic_dist.items():\n",
    "        # 计算该 Topic 至少需要的“人次”\n",
    "        quota = int(round(target_total_unique * ratio))\n",
    "        if quota == 0: quota = 1\n",
    "        \n",
    "        # 获取该 Topic 下的所有候选行\n",
    "        candidates = df_clean[df_clean['topic'] == topic]\n",
    "        \n",
    "        # 1. 检查【已选集合】里有多少能覆盖这个 Topic\n",
    "        # 这些文件虽然是在处理别的 Topic 时选进去的，但它们也能填当前 Topic 的坑\n",
    "        covered_by_existing = candidates[candidates['clean_name'].isin(selected_filenames_set)]\n",
    "        count_covered = len(covered_by_existing)\n",
    "        \n",
    "        needed = quota - count_covered\n",
    "        added_this_round = 0\n",
    "        \n",
    "        if needed > 0:\n",
    "            # 需要从【未选集合】里选新文件\n",
    "            pool = candidates[~candidates['clean_name'].isin(selected_filenames_set)]\n",
    "            \n",
    "            # 优先选 Zip 里的\n",
    "            pool_priority = pool[pool['in_groundtruth'] == True]\n",
    "            pool_normal = pool[pool['in_groundtruth'] == False]\n",
    "            \n",
    "            # 抽 Zip 里的\n",
    "            take_priority = min(len(pool_priority), needed)\n",
    "            if take_priority > 0:\n",
    "                picked = pool_priority.sample(n=take_priority, random_state=42)\n",
    "                selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                needed -= take_priority\n",
    "                added_this_round += take_priority\n",
    "                \n",
    "            # 抽 普通的\n",
    "            if needed > 0:\n",
    "                take_normal = min(len(pool_normal), needed)\n",
    "                if take_normal > 0:\n",
    "                    picked = pool_normal.sample(n=take_normal, random_state=42)\n",
    "                    selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                    needed -= take_normal\n",
    "                    added_this_round += take_normal\n",
    "        \n",
    "        stats_list.append({\n",
    "            'Topic': topic,\n",
    "            'Ratio': f\"{ratio:.2%}\",\n",
    "            'Quota_Slots': quota,\n",
    "            'Filled_By_Overlap': count_covered, # 被别的Topic顺带填上的\n",
    "            'Filled_By_New': added_this_round,  # 专门为此Topic新选的\n",
    "            'Total_Filled': count_covered + added_this_round\n",
    "        })\n",
    "\n",
    "    # --- 5. 第二轮：检查总数并兜底补齐 ---\n",
    "    current_unique_count = len(selected_filenames_set)\n",
    "    print(f\"\\n第一轮结束，当前选中唯一文件数: {current_unique_count}\")\n",
    "    \n",
    "    gap = target_total_unique - current_unique_count\n",
    "    \n",
    "    if gap > 0:\n",
    "        print(f\"Step 5: 数量未达标 (需 {target_total_unique}, 差 {gap})，执行随机补齐...\")\n",
    "        \n",
    "        # 从所有尚未被选中的文件中抽取\n",
    "        # 这里使用 df_clean 并在 clean_name 上去重，得到所有候选池\n",
    "        all_unique_candidates = df_clean.drop_duplicates(subset=['clean_name'])\n",
    "        remaining_pool = all_unique_candidates[~all_unique_candidates['clean_name'].isin(selected_filenames_set)]\n",
    "        \n",
    "        if len(remaining_pool) >= gap:\n",
    "            # 优先补 Zip 里的? 还是随机? 这里简单处理：优先 Zip\n",
    "            remaining_priority = remaining_pool[remaining_pool['in_groundtruth'] == True]\n",
    "            remaining_normal = remaining_pool[remaining_pool['in_groundtruth'] == False]\n",
    "            \n",
    "            top_up_list = []\n",
    "            \n",
    "            # 先拿剩下的 Zip\n",
    "            take_pri = min(len(remaining_priority), gap)\n",
    "            if take_pri > 0:\n",
    "                picked = remaining_priority.sample(n=take_pri, random_state=42)\n",
    "                selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                gap -= take_pri\n",
    "                \n",
    "            # 再拿剩下的普通\n",
    "            if gap > 0:\n",
    "                picked = remaining_normal.sample(n=gap, random_state=42)\n",
    "                selected_filenames_set.update(picked['clean_name'].tolist())\n",
    "                \n",
    "            print(\"  -> 补齐完成。\")\n",
    "        else:\n",
    "            print(f\"  -> Warning: 剩余可用文件不足，全部选入。最终数量: {current_unique_count + len(remaining_pool)}\")\n",
    "            selected_filenames_set.update(remaining_pool['clean_name'].tolist())\n",
    "\n",
    "    # --- 6. 生成最终 Excel 列表 ---\n",
    "    print(\"\\nStep 6: 生成最终文件列表...\")\n",
    "    \n",
    "    # 我们需要输出详细信息。因为一个文件可能对应多个 Topic，\n",
    "    # 为了 Excel 清爽，我们这里输出：文件名 | 对应的主Topic (或第一个) | 是否在Zip中\n",
    "    \n",
    "    # 这里的策略是：从原始 df_clean 中找出所有 clean_name 在 selected_filenames_set 里的行\n",
    "    # 并按照 clean_name 去重 (One File One Row)，确保输出只有 400 行\n",
    "    \n",
    "    # 先筛选出所有相关行\n",
    "    df_final_rows = df_clean[df_clean['clean_name'].isin(selected_filenames_set)].copy()\n",
    "    \n",
    "    # 为了展示清晰，我们对每个文件只保留一行 (keep='first')\n",
    "    # *注意*：这样输出的 Topic 列只是该文件众多 Topic 中的一个。\n",
    "    df_export_unique = df_final_rows.drop_duplicates(subset=['clean_name'], keep='first')\n",
    "    \n",
    "    # 分 Sheet\n",
    "    df_existing_export = df_export_unique[df_export_unique['in_groundtruth'] == True]\n",
    "    df_new_export = df_export_unique[df_export_unique['in_groundtruth'] == False]\n",
    "    \n",
    "    df_stats = pd.DataFrame(stats_list)\n",
    "\n",
    "    print(f\"写入结果到 {output_file}...\")\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_existing_export.to_excel(writer, sheet_name='1_Existing_In_Zip', index=False)\n",
    "        df_new_export.to_excel(writer, sheet_name='2_Files_To_Add', index=False)\n",
    "        df_stats.to_excel(writer, sheet_name='3_Distribution_Logic', index=False)\n",
    "        # 完整的 400 个文件列表\n",
    "        df_export_unique.to_excel(writer, sheet_name='4_Final_List_400', index=False)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"最终结果统计:\")\n",
    "    print(f\"唯一文件总数: {len(df_export_unique)} (目标: {target_total_unique})\")\n",
    "    print(f\"  - Zip内 (无需处理): {len(df_existing_export)}\")\n",
    "    print(f\"  - 新增 (需复制/下载): {len(df_new_export)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_stratified_dataset()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
