{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ce9a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data\n",
    "file_path = \"真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean bid amount and calculate log bid\n",
    "df['bid_amount'] = pd.to_numeric(df['涉及費用'].astype(str).str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "df['log_bid'] = np.log(df['bid_amount'])\n",
    "df = df.dropna(subset=['log_bid', '公司名稱', '大廈/屋苑名稱(入標年份)'])\n",
    "\n",
    "# 2. Calculate Residuals (De-centering)\n",
    "# Remove project-specific cost factors by subtracting the project mean\n",
    "df['project_mean'] = df.groupby('大廈/屋苑名稱(入標年份)')['log_bid'].transform('mean')\n",
    "df['residual'] = df['log_bid'] - df['project_mean']\n",
    "\n",
    "# 3. Filter Configuration\n",
    "MIN_JOINT_PROJECTS = 5      # Minimum joint bids required to calculate correlation\n",
    "CORRELATION_THRESHOLD = 0.8 # High risk threshold\n",
    "\n",
    "# 4. Build Pivot Matrix (Index=Project, Columns=Company, Values=Residual)\n",
    "pivot_matrix = df.pivot_table(index='大廈/屋苑名稱(入標年份)', columns='公司名稱', values='residual')\n",
    "\n",
    "# 5. Detection Algorithm\n",
    "suspicious_pairs = []\n",
    "companies = pivot_matrix.columns\n",
    "n_companies = len(companies)\n",
    "\n",
    "print(f\"Scanning interactions for {n_companies} companies...\")\n",
    "\n",
    "# Iterate through unique pairs of companies\n",
    "for i in range(n_companies):\n",
    "    for j in range(i + 1, n_companies):\n",
    "        firm_a = companies[i]\n",
    "        firm_b = companies[j]\n",
    "        \n",
    "        # Extract data for the pair\n",
    "        pair_data = pivot_matrix[[firm_a, firm_b]].dropna()\n",
    "        joint_count = len(pair_data)\n",
    "        \n",
    "        if joint_count >= MIN_JOINT_PROJECTS:\n",
    "            # Calculate correlation of residuals\n",
    "            corr = pair_data[firm_a].corr(pair_data[firm_b])\n",
    "            \n",
    "            if corr > CORRELATION_THRESHOLD:\n",
    "                suspicious_pairs.append({\n",
    "                    'Firm_A': firm_a,\n",
    "                    'Firm_B': firm_b,\n",
    "                    'Joint_Projects': joint_count,\n",
    "                    'Correlation': corr\n",
    "                })\n",
    "\n",
    "# 6. Output Results\n",
    "suspicious_df = pd.DataFrame(suspicious_pairs)\n",
    "\n",
    "if not suspicious_df.empty:\n",
    "    suspicious_df = suspicious_df.sort_values(by='Correlation', ascending=False)\n",
    "    print(\"\\n[High Risk Syndicates Found]\")\n",
    "    print(suspicious_df.head(100).to_markdown(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    suspicious_df.to_csv('suspicious_syndicates.csv', index=False)\n",
    "    print(\"\\nSaved to 'suspicious_syndicates.csv'\")\n",
    "else:\n",
    "    print(\"No pairs found exceeding the correlation threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1e5d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading & Cleaning\n",
    "# ==========================================\n",
    "file_path = \"傳真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# --- Cleaning A: Win Status ---\n",
    "# Standardize: 1 = Winner, 0 = Lost\n",
    "df['is_winner'] = pd.to_numeric(df['中標'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- Cleaning B: Rank ---\n",
    "# 1 = Lowest price, higher number = more expensive\n",
    "df['rank_num'] = pd.to_numeric(df['排名(平至貴)'], errors='coerce')\n",
    "\n",
    "# --- Cleaning C: Building Features ---\n",
    "df['has_mall'] = df['其他設施'].astype(str).str.contains('商場|mall', case=False, na=False).astype(int)\n",
    "df['has_club'] = df['其他設施'].astype(str).str.contains('會所|club', case=False, na=False).astype(int)\n",
    "# Extract unit count\n",
    "df['units_num'] = df['單位'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "print(f\"Data cleaning complete. Total records: {len(df)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Core Analysis Logic\n",
    "# ==========================================\n",
    "\n",
    "# Filter for winning bids only\n",
    "winning_bids = df[df['is_winner'] == 1].copy()\n",
    "\n",
    "# Aggregate company profile\n",
    "company_profile = winning_bids.groupby('公司名稱').agg({\n",
    "    '大廈/屋苑名稱(入標年份)': 'count',      # Win Count\n",
    "    'rank_num': 'mean',                   # Avg Winning Rank\n",
    "    'has_mall': 'mean',                   # Mall Ratio\n",
    "    'has_club': 'mean',                   # Club Ratio\n",
    "    'units_num': 'mean',                  # Avg Project Size (Units)\n",
    "    '公司性質': 'first'                   # Company Type\n",
    "}).rename(columns={'大廈/屋苑名稱(入標年份)': 'total_wins'})\n",
    "\n",
    "# Calculate Global Benchmarks\n",
    "global_avg_rank = winning_bids['rank_num'].mean()\n",
    "global_avg_units = winning_bids['units_num'].mean()\n",
    "print(f\"Global Benchmarks: Avg Rank={global_avg_rank:.2f}, Avg Units={global_avg_units:.0f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Define Risk Indicators\n",
    "# ==========================================\n",
    "\n",
    "# Filter: Companies with at least 2 wins\n",
    "suspects = company_profile[company_profile['total_wins'] >= 2].copy()\n",
    "\n",
    "# --- Indicator 1: Price Manipulation ---\n",
    "# Avg winning rank > 3 implies winning despite high prices\n",
    "suspects['Risk_HighPriceWin'] = suspects['rank_num'] > 3.0\n",
    "\n",
    "# --- Indicator 2: Predatory Targeting ---\n",
    "# Targeting large estates (>1000 units) with clubs (>30% ratio)\n",
    "suspects['Risk_BigTarget'] = (suspects['units_num'] > 1000) & (suspects['has_club'] > 0.3)\n",
    "\n",
    "# Combined High Risk Flag\n",
    "suspects['High_Risk_Flag'] = suspects['Risk_HighPriceWin'] | suspects['Risk_BigTarget']\n",
    "\n",
    "# ==========================================\n",
    "# 4. Output Results\n",
    "# ==========================================\n",
    "\n",
    "top_suspects = suspects[suspects['High_Risk_Flag'] == True].sort_values(by='rank_num', ascending=False)\n",
    "\n",
    "columns_to_show = ['total_wins', 'rank_num', 'units_num', 'has_club', 'Risk_HighPriceWin', 'Risk_BigTarget', '公司性質']\n",
    "print(\"\\n[High Risk Companies (Based on Feature Analysis)]\")\n",
    "print(top_suspects[columns_to_show])\n",
    "\n",
    "# Save to CSV\n",
    "top_suspects[columns_to_show].to_csv('high_risk_bidders_analysis.csv')\n",
    "print(\"\\nResults saved to 'high_risk_bidders_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a111d48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Data Preparation\n",
    "file_path = \"傳真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "df['bid_amount'] = pd.to_numeric(df['涉及費用'].astype(str).str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "df['log_bid'] = np.log(df['bid_amount'])\n",
    "df['is_winner'] = pd.to_numeric(df['中標'], errors='coerce').fillna(0).astype(int)\n",
    "df['rank_num'] = pd.to_numeric(df['排名(平至貴)'], errors='coerce')\n",
    "df['units_num'] = df['單位'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "# Extract district\n",
    "def extract_district(name):\n",
    "    districts = ['沙田', '荃灣', '土瓜灣', '深水埗', '灣仔', '西環', '大圍', '元朗', '青衣', '大埔', '九龍城', '上環', '中環']\n",
    "    for d in districts:\n",
    "        if d in str(name): return d\n",
    "    return 'Other'\n",
    "df['district'] = df['大廈/屋苑名稱(入標年份)'].apply(extract_district)\n",
    "\n",
    "# 2. Build Network Graph Data (based on residual correlation)\n",
    "# Calculate residuals\n",
    "df_reg = df.dropna(subset=['log_bid', '公司名稱', '大廈/屋苑名稱(入標年份)']).copy()\n",
    "df_reg['project_mean'] = df_reg.groupby('大廈/屋苑名稱(入標年份)')['log_bid'].transform('mean')\n",
    "df_reg['residual'] = df_reg['log_bid'] - df_reg['project_mean']\n",
    "\n",
    "# Pivot table\n",
    "pivot_matrix = df_reg.pivot_table(index='大廈/屋苑名稱(入標年份)', columns='公司名稱', values='residual')\n",
    "\n",
    "# Filter high-risk pairs (correlation > 0.8, common projects >= 3)\n",
    "corr_matrix = pivot_matrix.corr()\n",
    "links = []\n",
    "companies = corr_matrix.columns\n",
    "# For demonstration, take only active companies to avoid cluttered graph\n",
    "active_companies = df['公司名稱'].value_counts().head(30).index.tolist()\n",
    "# Ensure companies involved in the case are included\n",
    "garden_vista_companies = df[df['涉翠湖案'] == 1]['公司名稱'].unique()\n",
    "target_companies = list(set(active_companies) | set(garden_vista_companies))\n",
    "\n",
    "filtered_corr = pivot_matrix[target_companies].corr()\n",
    "\n",
    "# Build edge list\n",
    "for i in range(len(target_companies)):\n",
    "    for j in range(i+1, len(target_companies)):\n",
    "        firm_a = target_companies[i]\n",
    "        firm_b = target_companies[j]\n",
    "        # Check number of common projects\n",
    "        common_projects = pivot_matrix[[firm_a, firm_b]].dropna().shape[0]\n",
    "        if common_projects >= 3:\n",
    "            corr = filtered_corr.loc[firm_a, firm_b]\n",
    "            if corr > 0.8: # Threshold\n",
    "                links.append((firm_a, firm_b, corr))\n",
    "\n",
    "# 3. Plot 1: Collusion Network Graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "G = nx.Graph()\n",
    "for firm_a, firm_b, corr in links:\n",
    "    G.add_edge(firm_a, firm_b, weight=corr)\n",
    "\n",
    "# Node colors: case-involved companies in red, others in blue\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node in garden_vista_companies:\n",
    "        node_colors.append('#FF6B6B') # Red\n",
    "    else:\n",
    "        node_colors.append('#4ECDC4') # Teal\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors, alpha=0.9)\n",
    "nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.5, edge_color='gray')\n",
    "# Label optimization\n",
    "degrees = dict(G.degree)\n",
    "labels = {node: node for node in G.nodes() if degrees[node] > 1}\n",
    "# Set font for Chinese display\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "\n",
    "plt.title(\"Collusion Network: High Correlation Pairs (>0.8)\\n(Red nodes = Known Bad Companies)\", fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.savefig('collusion_network.png')\n",
    "\n",
    "# 4. Plot 2: Scatter Plot (Winning Rank vs Project Size)\n",
    "plt.figure(figsize=(10, 6))\n",
    "winners = df[df['is_winner'] == 1].dropna(subset=['units_num', 'rank_num'])\n",
    "# Mark case-involved companies\n",
    "winners['Type'] = winners['公司名稱'].apply(lambda x: 'High Risk (Case Involved)' if x in garden_vista_companies else 'Normal')\n",
    "\n",
    "sns.scatterplot(data=winners, x='units_num', y='rank_num', hue='Type', style='Type', \n",
    "                palette={'High Risk (Case Involved)': '#FF4500', 'Normal': '#1f77b4'}, s=100, alpha=0.7)\n",
    "\n",
    "plt.axhline(y=3, color='gray', linestyle='--', alpha=0.5, label='Risk Threshold (Rank > 3)')\n",
    "plt.axvline(x=1000, color='gray', linestyle='--', alpha=0.5, label='Target Threshold (Units > 1000)')\n",
    "plt.title(\"Winning Strategy Analysis: Are they targeting 'Fat Sheep'?\", fontsize=14)\n",
    "plt.xlabel(\"Project Size (Number of Units)\")\n",
    "plt.ylabel(\"Winning Price Rank (1=Cheapest)\")\n",
    "plt.legend(title='Company Type')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('winning_strategy_scatter.png')\n",
    "\n",
    "print(\"Charts generated: collusion_network.png, winning_strategy_scatter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9042ef4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def map_gvkey_to_cik():\n",
    "    # 1. Read data\n",
    "    print(\"Reading data...\")\n",
    "    try:\n",
    "        # Read main dataset\n",
    "        df_main = pd.read_csv('data_FraudDetection_JAR2020.csv')\n",
    "        # Read auxiliary dataset containing CIK\n",
    "        df_aaer = pd.read_csv('AAER_firm_year.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found. Please ensure the CSV files are in the current directory.\\nDetails: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Data preprocessing\n",
    "    # Ensure the data types of the join keys are consistent (usually p_aaer is numeric)\n",
    "    # In some cases, CSV reading may treat it as object, here force convert to numeric\n",
    "    df_main['p_aaer'] = pd.to_numeric(df_main['p_aaer'], errors='coerce')\n",
    "    df_aaer['P_AAER'] = pd.to_numeric(df_aaer['P_AAER'], errors='coerce')\n",
    "\n",
    "    # 3. Prepare mapping table\n",
    "    # The AAER file may have multiple rows for the same P_AAER (different years), but CIK should be the same.\n",
    "    # We only take P_AAER and CIK columns, and drop duplicates to prevent inflation of main data rows.\n",
    "    mapping_table = df_aaer[['P_AAER', 'CIK']].drop_duplicates()\n",
    "\n",
    "    # 4. Perform merge (Merge/Join)\n",
    "    # Use left join to retain all rows from main data\n",
    "    print(\"Merging data...\")\n",
    "    df_result = pd.merge(\n",
    "        df_main, \n",
    "        mapping_table, \n",
    "        left_on='p_aaer',   # Main table join key\n",
    "        right_on='P_AAER',  # Mapping table join key\n",
    "        how='left'          # Retain all rows from main data even if CIK is not found\n",
    "    )\n",
    "\n",
    "    # 5. Result statistics\n",
    "    total_rows = len(df_result)\n",
    "    matched_rows = df_result['CIK'].notna().sum()\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Processing completed!\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows successfully matched to CIK (fraud samples): {matched_rows}\")\n",
    "    print(f\"Rows not matched to CIK (non-fraud samples): {total_rows - matched_rows}\")\n",
    "    unique_cik_count = df_result['CIK'].dropna().nunique()\n",
    "    print(f\"Unique CIK count (deduplicated fraud samples): {unique_cik_count}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 6. View sample data\n",
    "    # Print a few rows of data successfully matched to CIK\n",
    "    print(\"\\nSample of successfully matched records (first 5 rows):\")\n",
    "    print(df_result[df_result['CIK'].notna()][['gvkey', 'p_aaer', 'CIK', 'misstate']].head())\n",
    "\n",
    "    # 7. Save results (optional)\n",
    "    df_result.to_csv('C:\\\\Users\\\\MaXin\\\\Desktop\\\\HSBC\\\\FraudDetection-master\\\\FraudDetection\\\\data_with_cik.csv', index=False)\n",
    "    print(\"\\nFile saved as data_with_cik.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_gvkey_to_cik()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761979e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Reorder columns to place 'CIK' right after 'gvkey'\n",
    "    cols = list(df_result.columns)\n",
    "    if 'gvkey' in cols and 'CIK' in cols:\n",
    "        cols.remove('CIK')\n",
    "        gvkey_index = cols.index('gvkey')\n",
    "        cols.insert(gvkey_index + 1, 'CIK')\n",
    "    df_result = df_result[cols]\n",
    "    \n",
    "    df_result.to_csv('data_with_cik.csv', index=False)\n",
    "    print(\"\\nFile saved as data_with_cik.csv\")\n",
    "    \n",
    "    # Save fraud-only rows to a new CSV\n",
    "    fraud_df = df_result[df_result['misstate'] == 1]\n",
    "    fraud_df.to_csv('fraud_only_data.csv', index=False)\n",
    "    print(\"Fraud-only data saved as fraud_only_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72366793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_ticker_matches(df):\n",
    "    # 1. 筛选出成功匹配到 Ticker 的行\n",
    "    # 注意：根据之前的逻辑，没匹配上的可能是 None 或 NaN\n",
    "    mask_matched = df['Ticker'].notna()\n",
    "    df_matched = df[mask_matched]\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 统计 1: 总体匹配情况\n",
    "    # -------------------------------------------------------\n",
    "    total_rows = len(df)\n",
    "    matched_count = len(df_matched)\n",
    "    unique_companies_matched = df_matched['CIK'].nunique()\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(\"DATASET TICKER 匹配统计报告\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"总行数 (Total Rows): {total_rows}\")\n",
    "    print(f\"成功匹配 Ticker 的行数: {matched_count}\")\n",
    "    print(f\"匹配率: {matched_count / total_rows:.2%}\")\n",
    "    print(f\"成功匹配的唯一公司数 (Unique Companies): {unique_companies_matched}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 统计 2: 造假样本 vs 非造假样本 的匹配情况\n",
    "    # -------------------------------------------------------\n",
    "    # 很多老旧的造假公司可能已经退市，导致匹配失败，这里看下差异\n",
    "    if 'misstate' in df.columns:\n",
    "        fraud_df = df[df['misstate'] == 1]\n",
    "        fraud_matched = fraud_df[fraud_df['Ticker'].notna()]\n",
    "        \n",
    "        print(f\"造假样本 (Fraud) 总数: {len(fraud_df)}\")\n",
    "        print(f\"造假样本匹配到 Ticker 数: {len(fraud_matched)}\")\n",
    "        print(f\"造假样本匹配率: {len(fraud_matched) / len(fraud_df) if len(fraud_df)>0 else 0:.2%}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 统计 3: 年份分布 (Year Distribution)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"匹配成功的年份分布 (前10个年份):\")\n",
    "    year_dist = df_matched['fyear'].value_counts().sort_index()\n",
    "    print(year_dist.head(10))\n",
    "    print(\"...\")\n",
    "    print(year_dist.tail(5))\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 可视化 (可选)\n",
    "    # -------------------------------------------------------\n",
    "    # 绘制简单的柱状图看分布\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(year_dist.index, year_dist.values, color='skyblue', label='Matched Rows')\n",
    "    plt.title('Distribution of Rows with Matched Tickers by Fiscal Year')\n",
    "    plt.xlabel('Fiscal Year (fyear)')\n",
    "    plt.ylabel('Count of Matched Rows')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 运行分析\n",
    "analyze_ticker_matches(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f23daa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def thorough_comparison(my_file, colleague_file):\n",
    "    df_mine = pd.read_excel(my_file)\n",
    "    df_theirs = pd.read_excel(colleague_file)\n",
    "\n",
    "    df_m = df_mine[df_mine['Ticker'].notna()].copy()\n",
    "    df_t = df_theirs[df_theirs['Ticker'].notna()].copy()\n",
    "\n",
    "    for df in [df_m, df_t]:\n",
    "        df['CIK'] = pd.to_numeric(df['CIK'], errors='coerce').fillna(0).astype(int)\n",
    "        df['key'] = df['CIK'].astype(str) + \"_\" + df['fyear'].astype(str)\n",
    "\n",
    "    ciks_mine = set(df_m['CIK'])\n",
    "    ciks_theirs = set(df_t['CIK'])\n",
    "\n",
    "    only_mine_ciks = ciks_mine - ciks_theirs\n",
    "    only_theirs_ciks = ciks_theirs - ciks_mine\n",
    "    common_ciks = ciks_mine & ciks_theirs\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"【1. 唯一公司 (CIK) 维度对比】\")\n",
    "    print(f\"你的唯一公司数: {len(ciks_mine)}\")\n",
    "    print(f\"同事的唯一公司数: {len(ciks_theirs)}\")\n",
    "    print(f\"共同匹配到的公司数: {len(common_ciks)}\")\n",
    "    print(f\"仅你匹配到的公司数: {len(only_mine_ciks)}\")\n",
    "    print(f\"仅同事匹配到的公司数: {len(only_theirs_ciks)}\")\n",
    "\n",
    "    if only_mine_ciks:\n",
    "        print(\"\\n>>> 仅在你结果中出现的公司 (示例):\")\n",
    "        print(df_m[df_m['CIK'].isin(only_mine_ciks)].drop_duplicates('CIK')[['CIK', 'Ticker']].head(10))\n",
    "\n",
    "    if only_theirs_ciks:\n",
    "        print(\"\\n>>> 仅在同事结果中出现的公司 (示例):\")\n",
    "        print(df_t[df_t['CIK'].isin(only_theirs_ciks)].drop_duplicates('CIK')[['CIK', 'Ticker']].head(10))\n",
    "\n",
    "    keys_mine = set(df_m['key'])\n",
    "    keys_theirs = set(df_t['key'])\n",
    "\n",
    "    only_mine_keys = keys_mine - keys_theirs\n",
    "    only_theirs_keys = keys_theirs - keys_mine\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"【2. 样本行 (CIK-fyear) 维度对比】\")\n",
    "    print(f\"仅你有的样本行数: {len(only_mine_keys)}\")\n",
    "    print(f\"仅同事有的样本行数: {len(only_theirs_keys)}\")\n",
    "\n",
    "    merged = pd.merge(df_m[['key', 'Ticker', 'fyear', 'CIK']], \n",
    "                      df_t[['key', 'Ticker']], \n",
    "                      on='key', suffixes=('_mine', '_theirs'))\n",
    "    \n",
    "    conflicts = merged[merged['Ticker_mine'] != merged['Ticker_theirs']]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"【3. Ticker 标注冲突对比】\")\n",
    "    if not conflicts.empty:\n",
    "        print(f\"发现 {len(conflicts)} 行数据的 Ticker 标注不一致：\")\n",
    "        print(conflicts[['fyear', 'CIK', 'Ticker_mine', 'Ticker_theirs']])\n",
    "    else:\n",
    "        print(\"恭喜！所有共同样本的 Ticker 标注完全一致。\")\n",
    "\n",
    "# 使用方法\n",
    "my_file = 'data_with_ticker_and_name.xlsx'\n",
    "colleague_file = 'colleague_data.xlsx'\n",
    "thorough_comparison(my_file, colleague_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7188b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "batches = ['1', '2', '3']\n",
    "target_variables = ['a_1', 'b_2', 'c_3', 'is_active'] \n",
    "output_file = 'variable_stats_analysis.xlsx'\n",
    "                          \n",
    "results_list = []\n",
    "\n",
    "for run_dt in batches:\n",
    "    print(f\"Processing batch: {run_dt}\")\n",
    "    \n",
    "    non_fraud_df = mock_load_data(run_dt, is_fraud=False)\n",
    "    \n",
    "    data_groups = {\n",
    "        'non-fraud': non_fraud_df\n",
    "    }\n",
    "    \n",
    "    combined_df = non_fraud_df\n",
    "\n",
    "    # Fraud part\n",
    "    # fraud_df = mock_load_data(run_dt, is_fraud=True)\n",
    "    # data_groups['fraud'] = fraud_df\n",
    "    # combined_df = pd.concat([non_fraud_df, fraud_df], ignore_index=True)\n",
    "\n",
    "    # Total part\n",
    "    # data_groups['total'] = combined_df\n",
    "\n",
    "    for var in target_variables:\n",
    "\n",
    "        if var not in combined_df.columns:\n",
    "            print(f\"Warning: Col {var} is not existing, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        is_numeric = pd.api.types.is_numeric_dtype(combined_df[var])\n",
    "        is_bool = pd.api.types.is_bool_dtype(combined_df[var])\n",
    "        \n",
    "        if not is_numeric or is_bool:\n",
    "            print(f\"Col {var} is not numeric, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        for group_name, df_group in data_groups.items():\n",
    "            series = df_group[var]\n",
    "            \n",
    "            stats = {\n",
    "                'var': var,\n",
    "                'batch': run_dt,\n",
    "                'Flag_fraud': group_name,\n",
    "                'mean': series.mean(),\n",
    "                'std': series.std(),\n",
    "                'min': series.min(),\n",
    "                'median': series.median(),\n",
    "                'max': series.max()\n",
    "            }\n",
    "            results_list.append(stats)\n",
    "\n",
    "if results_list:\n",
    "    final_df = pd.DataFrame(results_list)\n",
    "\n",
    "    columns_order = ['var', 'batch', 'Flag_fraud', 'mean', 'std', 'min', 'median', 'max']\n",
    "    final_df = final_df[columns_order]\n",
    "\n",
    "    group_order_map = {'fraud': 1, 'non-fraud': 2, 'total': 3}\n",
    "    final_df['group_rank'] = final_df['Flag_fraud'].map(group_order_map)\n",
    "\n",
    "    final_df = final_df.sort_values(by=['var', 'batch', 'group_rank'])\n",
    "    final_df = final_df.drop(columns=['group_rank'])\n",
    "\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    print(final_df)\n",
    "\n",
    "    try:\n",
    "        final_df.to_excel(output_file, index=False)\n",
    "        print(f\"\\nResults are saved in {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to save Excel: {e}\")\n",
    "else:\n",
    "    print(\"No valid results to display or save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4195f6c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. 辅助工具函数\n",
    "# ==========================================\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"自然排序键生成 (使用 Tuple 解决 unhashable 问题)\"\"\"\n",
    "    return tuple(int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', str(s)))\n",
    "\n",
    "def get_top_5_values(series):\n",
    "    \"\"\"获取 Series 中出现频率最高的5个值，作为类型判断的证据\"\"\"\n",
    "    try:\n",
    "        # 转换为字符串以免混合类型报错，取前5个\n",
    "        return str(series.value_counts().head(5).index.tolist())\n",
    "    except:\n",
    "        return \"Error getting values\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. 模拟数据生成 (增加了一些 tricky 的情况)\n",
    "# ==========================================\n",
    "def mock_load_data(run_dt, is_fraud=False):\n",
    "    np.random.seed(int(run_dt) + int(is_fraud))\n",
    "    n_rows = 50\n",
    "    data = {\n",
    "        # --- 数值变量 ---\n",
    "        'amount': np.random.uniform(10, 100, n_rows),\n",
    "        'age': np.random.randint(20, 60, n_rows),\n",
    "        \n",
    "        # --- 布尔变量 (标准) ---\n",
    "        'is_active': np.random.choice([True, False], n_rows),\n",
    "        'has_email': np.random.choice([True, False, None], n_rows), # 含 Null\n",
    "        \n",
    "        # --- 容易混淆的变量 (测试诊断功能) ---\n",
    "        'flag_str_bool': np.random.choice(['True', 'False'], n_rows), # 字符串布尔\n",
    "        'category_code': np.random.choice(['A1', 'B2', 'C3'], n_rows), # 字符串\n",
    "        'mixed_num': np.random.choice([1, 0, 1, 0], n_rows) # 整数 0/1 (通常被视为数值，但逻辑上可能是布尔)\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# ==========================================\n",
    "# 3. 配置部分\n",
    "# ==========================================\n",
    "batches = ['1', '2', '3']\n",
    "output_file = 'comprehensive_analysis.xlsx'\n",
    "\n",
    "# 容器，用于存放最终结果\n",
    "numeric_results = []\n",
    "boolean_results = []\n",
    "diagnosis_results = []\n",
    "\n",
    "# ==========================================\n",
    "# 4. 主处理逻辑\n",
    "# ==========================================\n",
    "\n",
    "for run_dt in batches:\n",
    "    print(f\"Processing batch: {run_dt}\")\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    non_fraud_df = mock_load_data(run_dt, is_fraud=False)\n",
    "    \n",
    "    # 构建数据组 (可扩展 fraud/total)\n",
    "    data_groups = {'non-fraud': non_fraud_df}\n",
    "    combined_df = non_fraud_df # 用于类型检查和诊断\n",
    "    \n",
    "    # Fraud / Total 部分 (按需开启)\n",
    "    # fraud_df = mock_load_data(run_dt, is_fraud=True)\n",
    "    # data_groups['fraud'] = fraud_df\n",
    "    # combined_df = pd.concat([non_fraud_df, fraud_df], ignore_index=True)\n",
    "    # data_groups['total'] = combined_df\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 步骤 A: 变量诊断 (针对该 batch 的所有变量)\n",
    "    # -------------------------------------------------------\n",
    "    # 我们对 combined_df 的每一列进行诊断，帮助你发现遗漏的变量\n",
    "    for col in combined_df.columns:\n",
    "        # 判断类型\n",
    "        is_num = pd.api.types.is_numeric_dtype(combined_df[col])\n",
    "        is_bool = pd.api.types.is_bool_dtype(combined_df[col])\n",
    "        dtype_name = str(combined_df[col].dtype)\n",
    "        \n",
    "        # 收集证据 (Top 5 values)\n",
    "        evidence = get_top_5_values(combined_df[col])\n",
    "        \n",
    "        diag_info = {\n",
    "            'batch': run_dt,\n",
    "            'var': col,\n",
    "            'pandas_dtype': dtype_name,\n",
    "            'is_numeric_check': is_num,\n",
    "            'is_boolean_check': is_bool,\n",
    "            'top_5_values_evidence': evidence\n",
    "        }\n",
    "        diagnosis_results.append(diag_info)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 步骤 B: 变量分析 (自动分流 Numeric 和 Boolean)\n",
    "    # -------------------------------------------------------\n",
    "    # 这里我们遍历 dataframe 的所有列，或者你指定的 target_variables\n",
    "    # 建议：这里遍历 all columns，然后根据类型自动决定去哪个表\n",
    "    \n",
    "    target_vars = combined_df.columns # 自动分析所有列\n",
    "    \n",
    "    for var in target_vars:\n",
    "        \n",
    "        # 获取 Series 用于判断\n",
    "        series_check = combined_df[var]\n",
    "        \n",
    "        is_bool = pd.api.types.is_bool_dtype(series_check)\n",
    "        is_numeric = pd.api.types.is_numeric_dtype(series_check)\n",
    "        \n",
    "        # --- 分支 1: 处理 Boolean 变量 ---\n",
    "        if is_bool:\n",
    "            for group_name, df_group in data_groups.items():\n",
    "                s = df_group[var].dropna() # 去除 NA 进行统计\n",
    "                \n",
    "                if len(s) == 0:\n",
    "                    continue\n",
    "\n",
    "                # 计算 True/False 分布\n",
    "                val_counts = s.value_counts(normalize=True) # 获取百分比\n",
    "                count_unique = s.nunique()\n",
    "                \n",
    "                try:\n",
    "                    most_freq = s.value_counts().idxmax()\n",
    "                except:\n",
    "                    most_freq = np.nan\n",
    "                \n",
    "                # 安全获取 True/False 的百分比 (有些列可能全为 True 或全为 False)\n",
    "                pct_true = val_counts.get(True, 0.0)\n",
    "                pct_false = val_counts.get(False, 0.0)\n",
    "\n",
    "                bool_stats = {\n",
    "                    'var': var,\n",
    "                    'batch': run_dt,\n",
    "                    'Flag_fraud': group_name,\n",
    "                    'number_of_unique': count_unique,\n",
    "                    'most_frequent_value': most_freq,\n",
    "                    '%_of_True': pct_true,\n",
    "                    '%_of_False': pct_false\n",
    "                }\n",
    "                boolean_results.append(bool_stats)\n",
    "\n",
    "        # --- 分支 2: 处理 Numeric 变量 (排除 Boolean) ---\n",
    "        elif is_numeric: \n",
    "            for group_name, df_group in data_groups.items():\n",
    "                s = df_group[var]\n",
    "                \n",
    "                num_stats = {\n",
    "                    'var': var,\n",
    "                    'batch': run_dt,\n",
    "                    'Flag_fraud': group_name,\n",
    "                    'mean': s.mean(),\n",
    "                    'std': s.std(),\n",
    "                    'min': s.min(),\n",
    "                    'median': s.median(),\n",
    "                    'max': s.max()\n",
    "                }\n",
    "                numeric_results.append(num_stats)\n",
    "        \n",
    "        # --- 分支 3: 其他类型 (String/Object) ---\n",
    "        else:\n",
    "            # 这些变量会被跳过，但它们已经记录在 Diagnosis Sheet 里了\n",
    "            pass\n",
    "\n",
    "# ==========================================\n",
    "# 5. 结果整合与保存\n",
    "# ==========================================\n",
    "\n",
    "print(\"正在生成报告...\")\n",
    "\n",
    "# --- 处理 Numerical 结果 ---\n",
    "if numeric_results:\n",
    "    df_num = pd.DataFrame(numeric_results)\n",
    "    \n",
    "    # 格式化数值为字符串 (保留2位小数)\n",
    "    for col in ['mean', 'std', 'min', 'median', 'max']:\n",
    "        df_num[col] = df_num[col].map('{:.2f}'.format)\n",
    "        \n",
    "    # 排序\n",
    "    df_num['sort_key'] = df_num['var'].apply(natural_sort_key)\n",
    "    group_map = {'fraud': 1, 'non-fraud': 2, 'total': 3}\n",
    "    df_num['g_rank'] = df_num['Flag_fraud'].map(group_map)\n",
    "    df_num = df_num.sort_values(by=['sort_key', 'batch', 'g_rank']).drop(columns=['sort_key', 'g_rank'])\n",
    "else:\n",
    "    df_num = pd.DataFrame()\n",
    "\n",
    "# --- 处理 Boolean 结果 ---\n",
    "if boolean_results:\n",
    "    df_bool = pd.DataFrame(boolean_results)\n",
    "    \n",
    "    cols_order_bool = ['var', 'batch', 'Flag_fraud', 'number_of_unique', 'most_frequent_value', '%_of_True', '%_of_False']\n",
    "    df_bool = df_bool[cols_order_bool]\n",
    "    \n",
    "    # 格式化百分比 (例如 0.5 -> \"50.00%\")\n",
    "    for col in ['%_of_True', '%_of_False']:\n",
    "        df_bool[col] = df_bool[col].apply(lambda x: \"{:.2f}%\".format(x * 100))\n",
    "    \n",
    "    # 排序\n",
    "    df_bool['sort_key'] = df_bool['var'].apply(natural_sort_key)\n",
    "    df_bool['g_rank'] = df_bool['Flag_fraud'].map(group_map)\n",
    "    df_bool = df_bool.sort_values(by=['sort_key', 'batch', 'g_rank']).drop(columns=['sort_key', 'g_rank'])\n",
    "else:\n",
    "    df_bool = pd.DataFrame()\n",
    "\n",
    "# --- 处理 Diagnosis 结果 ---\n",
    "if diagnosis_results:\n",
    "    df_diag = pd.DataFrame(diagnosis_results)\n",
    "    cols_diag = ['var', 'batch', 'pandas_dtype', 'is_numeric_check', 'is_boolean_check', 'top_5_values_evidence']\n",
    "    df_diag = df_diag[cols_diag]\n",
    "    # 按变量名排序\n",
    "    df_diag['sort_key'] = df_diag['var'].apply(natural_sort_key)\n",
    "    df_diag = df_diag.sort_values(by=['sort_key', 'batch']).drop(columns=['sort_key'])\n",
    "else:\n",
    "    df_diag = pd.DataFrame()\n",
    "\n",
    "# --- 写入 Excel (多 Sheet) ---\n",
    "try:\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        \n",
    "        # Sheet 1: 数值分析\n",
    "        if not df_num.empty:\n",
    "            df_num.to_excel(writer, sheet_name='Numerical_Analysis', index=False)\n",
    "            print(f\"- Sheet 'Numerical_Analysis' created ({len(df_num)} rows).\")\n",
    "        \n",
    "        # Sheet 2: 布尔分析\n",
    "        if not df_bool.empty:\n",
    "            df_bool.to_excel(writer, sheet_name='Boolean_Analysis', index=False)\n",
    "            print(f\"- Sheet 'Boolean_Analysis' created ({len(df_bool)} rows).\")\n",
    "            \n",
    "        # Sheet 3: 变量诊断 (证据)\n",
    "        if not df_diag.empty:\n",
    "            df_diag.to_excel(writer, sheet_name='Variable_Diagnosis', index=False)\n",
    "            # 调整列宽以便阅读证据\n",
    "            worksheet = writer.sheets['Variable_Diagnosis']\n",
    "            worksheet.set_column('F:F', 50) # 拉宽 'top_5_values_evidence' 列\n",
    "            print(f\"- Sheet 'Variable_Diagnosis' created ({len(df_diag)} rows).\")\n",
    "            \n",
    "    print(f\"\\n成功! 文件已保存至: {output_file}\")\n",
    "    \n",
    "    # 简单打印一下诊断结果供预览\n",
    "    print(\"\\n--- 变量类型诊断预览 (部分) ---\")\n",
    "    print(df_diag.head(10).to_string(index=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"保存失败: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
