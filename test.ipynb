{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ce9a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Prepare Data\n",
    "file_path = \"真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Clean bid amount and calculate log bid\n",
    "df['bid_amount'] = pd.to_numeric(df['涉及費用'].astype(str).str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "df['log_bid'] = np.log(df['bid_amount'])\n",
    "df = df.dropna(subset=['log_bid', '公司名稱', '大廈/屋苑名稱(入標年份)'])\n",
    "\n",
    "# 2. Calculate Residuals (De-centering)\n",
    "# Remove project-specific cost factors by subtracting the project mean\n",
    "df['project_mean'] = df.groupby('大廈/屋苑名稱(入標年份)')['log_bid'].transform('mean')\n",
    "df['residual'] = df['log_bid'] - df['project_mean']\n",
    "\n",
    "# 3. Filter Configuration\n",
    "MIN_JOINT_PROJECTS = 5      # Minimum joint bids required to calculate correlation\n",
    "CORRELATION_THRESHOLD = 0.8 # High risk threshold\n",
    "\n",
    "# 4. Build Pivot Matrix (Index=Project, Columns=Company, Values=Residual)\n",
    "pivot_matrix = df.pivot_table(index='大廈/屋苑名稱(入標年份)', columns='公司名稱', values='residual')\n",
    "\n",
    "# 5. Detection Algorithm\n",
    "suspicious_pairs = []\n",
    "companies = pivot_matrix.columns\n",
    "n_companies = len(companies)\n",
    "\n",
    "print(f\"Scanning interactions for {n_companies} companies...\")\n",
    "\n",
    "# Iterate through unique pairs of companies\n",
    "for i in range(n_companies):\n",
    "    for j in range(i + 1, n_companies):\n",
    "        firm_a = companies[i]\n",
    "        firm_b = companies[j]\n",
    "        \n",
    "        # Extract data for the pair\n",
    "        pair_data = pivot_matrix[[firm_a, firm_b]].dropna()\n",
    "        joint_count = len(pair_data)\n",
    "        \n",
    "        if joint_count >= MIN_JOINT_PROJECTS:\n",
    "            # Calculate correlation of residuals\n",
    "            corr = pair_data[firm_a].corr(pair_data[firm_b])\n",
    "            \n",
    "            if corr > CORRELATION_THRESHOLD:\n",
    "                suspicious_pairs.append({\n",
    "                    'Firm_A': firm_a,\n",
    "                    'Firm_B': firm_b,\n",
    "                    'Joint_Projects': joint_count,\n",
    "                    'Correlation': corr\n",
    "                })\n",
    "\n",
    "# 6. Output Results\n",
    "suspicious_df = pd.DataFrame(suspicious_pairs)\n",
    "\n",
    "if not suspicious_df.empty:\n",
    "    suspicious_df = suspicious_df.sort_values(by='Correlation', ascending=False)\n",
    "    print(\"\\n[High Risk Syndicates Found]\")\n",
    "    print(suspicious_df.head(100).to_markdown(index=False))\n",
    "    \n",
    "    # Save to CSV\n",
    "    suspicious_df.to_csv('suspicious_syndicates.csv', index=False)\n",
    "    print(\"\\nSaved to 'suspicious_syndicates.csv'\")\n",
    "else:\n",
    "    print(\"No pairs found exceeding the correlation threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae1e5d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Loading & Cleaning\n",
    "# ==========================================\n",
    "file_path = \"傳真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# --- Cleaning A: Win Status ---\n",
    "# Standardize: 1 = Winner, 0 = Lost\n",
    "df['is_winner'] = pd.to_numeric(df['中標'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# --- Cleaning B: Rank ---\n",
    "# 1 = Lowest price, higher number = more expensive\n",
    "df['rank_num'] = pd.to_numeric(df['排名(平至貴)'], errors='coerce')\n",
    "\n",
    "# --- Cleaning C: Building Features ---\n",
    "df['has_mall'] = df['其他設施'].astype(str).str.contains('商場|mall', case=False, na=False).astype(int)\n",
    "df['has_club'] = df['其他設施'].astype(str).str.contains('會所|club', case=False, na=False).astype(int)\n",
    "# Extract unit count\n",
    "df['units_num'] = df['單位'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "print(f\"Data cleaning complete. Total records: {len(df)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Core Analysis Logic\n",
    "# ==========================================\n",
    "\n",
    "# Filter for winning bids only\n",
    "winning_bids = df[df['is_winner'] == 1].copy()\n",
    "\n",
    "# Aggregate company profile\n",
    "company_profile = winning_bids.groupby('公司名稱').agg({\n",
    "    '大廈/屋苑名稱(入標年份)': 'count',      # Win Count\n",
    "    'rank_num': 'mean',                   # Avg Winning Rank\n",
    "    'has_mall': 'mean',                   # Mall Ratio\n",
    "    'has_club': 'mean',                   # Club Ratio\n",
    "    'units_num': 'mean',                  # Avg Project Size (Units)\n",
    "    '公司性質': 'first'                   # Company Type\n",
    "}).rename(columns={'大廈/屋苑名稱(入標年份)': 'total_wins'})\n",
    "\n",
    "# Calculate Global Benchmarks\n",
    "global_avg_rank = winning_bids['rank_num'].mean()\n",
    "global_avg_units = winning_bids['units_num'].mean()\n",
    "print(f\"Global Benchmarks: Avg Rank={global_avg_rank:.2f}, Avg Units={global_avg_units:.0f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. Define Risk Indicators\n",
    "# ==========================================\n",
    "\n",
    "# Filter: Companies with at least 2 wins\n",
    "suspects = company_profile[company_profile['total_wins'] >= 2].copy()\n",
    "\n",
    "# --- Indicator 1: Price Manipulation ---\n",
    "# Avg winning rank > 3 implies winning despite high prices\n",
    "suspects['Risk_HighPriceWin'] = suspects['rank_num'] > 3.0\n",
    "\n",
    "# --- Indicator 2: Predatory Targeting ---\n",
    "# Targeting large estates (>1000 units) with clubs (>30% ratio)\n",
    "suspects['Risk_BigTarget'] = (suspects['units_num'] > 1000) & (suspects['has_club'] > 0.3)\n",
    "\n",
    "# Combined High Risk Flag\n",
    "suspects['High_Risk_Flag'] = suspects['Risk_HighPriceWin'] | suspects['Risk_BigTarget']\n",
    "\n",
    "# ==========================================\n",
    "# 4. Output Results\n",
    "# ==========================================\n",
    "\n",
    "top_suspects = suspects[suspects['High_Risk_Flag'] == True].sort_values(by='rank_num', ascending=False)\n",
    "\n",
    "columns_to_show = ['total_wins', 'rank_num', 'units_num', 'has_club', 'Risk_HighPriceWin', 'Risk_BigTarget', '公司性質']\n",
    "print(\"\\n[High Risk Companies (Based on Feature Analysis)]\")\n",
    "print(top_suspects[columns_to_show])\n",
    "\n",
    "# Save to CSV\n",
    "top_suspects[columns_to_show].to_csv('high_risk_bidders_analysis.csv')\n",
    "print(\"\\nResults saved to 'high_risk_bidders_analysis.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a111d48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Data Preparation\n",
    "file_path = \"傳真社製首個樓宇維修公開資料庫.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "df['bid_amount'] = pd.to_numeric(df['涉及費用'].astype(str).str.replace(r'[^\\d.]', '', regex=True), errors='coerce')\n",
    "df['log_bid'] = np.log(df['bid_amount'])\n",
    "df['is_winner'] = pd.to_numeric(df['中標'], errors='coerce').fillna(0).astype(int)\n",
    "df['rank_num'] = pd.to_numeric(df['排名(平至貴)'], errors='coerce')\n",
    "df['units_num'] = df['單位'].astype(str).str.extract(r'(\\d+)').astype(float)\n",
    "\n",
    "# Extract district\n",
    "def extract_district(name):\n",
    "    districts = ['沙田', '荃灣', '土瓜灣', '深水埗', '灣仔', '西環', '大圍', '元朗', '青衣', '大埔', '九龍城', '上環', '中環']\n",
    "    for d in districts:\n",
    "        if d in str(name): return d\n",
    "    return 'Other'\n",
    "df['district'] = df['大廈/屋苑名稱(入標年份)'].apply(extract_district)\n",
    "\n",
    "# 2. Build Network Graph Data (based on residual correlation)\n",
    "# Calculate residuals\n",
    "df_reg = df.dropna(subset=['log_bid', '公司名稱', '大廈/屋苑名稱(入標年份)']).copy()\n",
    "df_reg['project_mean'] = df_reg.groupby('大廈/屋苑名稱(入標年份)')['log_bid'].transform('mean')\n",
    "df_reg['residual'] = df_reg['log_bid'] - df_reg['project_mean']\n",
    "\n",
    "# Pivot table\n",
    "pivot_matrix = df_reg.pivot_table(index='大廈/屋苑名稱(入標年份)', columns='公司名稱', values='residual')\n",
    "\n",
    "# Filter high-risk pairs (correlation > 0.8, common projects >= 3)\n",
    "corr_matrix = pivot_matrix.corr()\n",
    "links = []\n",
    "companies = corr_matrix.columns\n",
    "# For demonstration, take only active companies to avoid cluttered graph\n",
    "active_companies = df['公司名稱'].value_counts().head(30).index.tolist()\n",
    "# Ensure companies involved in the case are included\n",
    "garden_vista_companies = df[df['涉翠湖案'] == 1]['公司名稱'].unique()\n",
    "target_companies = list(set(active_companies) | set(garden_vista_companies))\n",
    "\n",
    "filtered_corr = pivot_matrix[target_companies].corr()\n",
    "\n",
    "# Build edge list\n",
    "for i in range(len(target_companies)):\n",
    "    for j in range(i+1, len(target_companies)):\n",
    "        firm_a = target_companies[i]\n",
    "        firm_b = target_companies[j]\n",
    "        # Check number of common projects\n",
    "        common_projects = pivot_matrix[[firm_a, firm_b]].dropna().shape[0]\n",
    "        if common_projects >= 3:\n",
    "            corr = filtered_corr.loc[firm_a, firm_b]\n",
    "            if corr > 0.8: # Threshold\n",
    "                links.append((firm_a, firm_b, corr))\n",
    "\n",
    "# 3. Plot 1: Collusion Network Graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "G = nx.Graph()\n",
    "for firm_a, firm_b, corr in links:\n",
    "    G.add_edge(firm_a, firm_b, weight=corr)\n",
    "\n",
    "# Node colors: case-involved companies in red, others in blue\n",
    "node_colors = []\n",
    "for node in G.nodes():\n",
    "    if node in garden_vista_companies:\n",
    "        node_colors.append('#FF6B6B') # Red\n",
    "    else:\n",
    "        node_colors.append('#4ECDC4') # Teal\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G, k=0.3, iterations=50, seed=42)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors, alpha=0.9)\n",
    "nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.5, edge_color='gray')\n",
    "# Label optimization\n",
    "degrees = dict(G.degree)\n",
    "labels = {node: node for node in G.nodes() if degrees[node] > 1}\n",
    "# Set font for Chinese display\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "\n",
    "plt.title(\"Collusion Network: High Correlation Pairs (>0.8)\\n(Red nodes = Known Bad Companies)\", fontsize=15)\n",
    "plt.axis('off')\n",
    "plt.savefig('collusion_network.png')\n",
    "\n",
    "# 4. Plot 2: Scatter Plot (Winning Rank vs Project Size)\n",
    "plt.figure(figsize=(10, 6))\n",
    "winners = df[df['is_winner'] == 1].dropna(subset=['units_num', 'rank_num'])\n",
    "# Mark case-involved companies\n",
    "winners['Type'] = winners['公司名稱'].apply(lambda x: 'High Risk (Case Involved)' if x in garden_vista_companies else 'Normal')\n",
    "\n",
    "sns.scatterplot(data=winners, x='units_num', y='rank_num', hue='Type', style='Type', \n",
    "                palette={'High Risk (Case Involved)': '#FF4500', 'Normal': '#1f77b4'}, s=100, alpha=0.7)\n",
    "\n",
    "plt.axhline(y=3, color='gray', linestyle='--', alpha=0.5, label='Risk Threshold (Rank > 3)')\n",
    "plt.axvline(x=1000, color='gray', linestyle='--', alpha=0.5, label='Target Threshold (Units > 1000)')\n",
    "plt.title(\"Winning Strategy Analysis: Are they targeting 'Fat Sheep'?\", fontsize=14)\n",
    "plt.xlabel(\"Project Size (Number of Units)\")\n",
    "plt.ylabel(\"Winning Price Rank (1=Cheapest)\")\n",
    "plt.legend(title='Company Type')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('winning_strategy_scatter.png')\n",
    "\n",
    "print(\"Charts generated: collusion_network.png, winning_strategy_scatter.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9042ef4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def map_gvkey_to_cik():\n",
    "    # 1. Read data\n",
    "    print(\"Reading data...\")\n",
    "    try:\n",
    "        # Read main dataset\n",
    "        df_main = pd.read_csv('data_FraudDetection_JAR2020.csv')\n",
    "        # Read auxiliary dataset containing CIK\n",
    "        df_aaer = pd.read_csv('AAER_firm_year.csv')\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found. Please ensure the CSV files are in the current directory.\\nDetails: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Data preprocessing\n",
    "    # Ensure the data types of the join keys are consistent (usually p_aaer is numeric)\n",
    "    # In some cases, CSV reading may treat it as object, here force convert to numeric\n",
    "    df_main['p_aaer'] = pd.to_numeric(df_main['p_aaer'], errors='coerce')\n",
    "    df_aaer['P_AAER'] = pd.to_numeric(df_aaer['P_AAER'], errors='coerce')\n",
    "\n",
    "    # 3. Prepare mapping table\n",
    "    # The AAER file may have multiple rows for the same P_AAER (different years), but CIK should be the same.\n",
    "    # We only take P_AAER and CIK columns, and drop duplicates to prevent inflation of main data rows.\n",
    "    mapping_table = df_aaer[['P_AAER', 'CIK']].drop_duplicates()\n",
    "\n",
    "    # 4. Perform merge (Merge/Join)\n",
    "    # Use left join to retain all rows from main data\n",
    "    print(\"Merging data...\")\n",
    "    df_result = pd.merge(\n",
    "        df_main, \n",
    "        mapping_table, \n",
    "        left_on='p_aaer',   # Main table join key\n",
    "        right_on='P_AAER',  # Mapping table join key\n",
    "        how='left'          # Retain all rows from main data even if CIK is not found\n",
    "    )\n",
    "\n",
    "    # 5. Result statistics\n",
    "    total_rows = len(df_result)\n",
    "    matched_rows = df_result['CIK'].notna().sum()\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Processing completed!\")\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Rows successfully matched to CIK (fraud samples): {matched_rows}\")\n",
    "    print(f\"Rows not matched to CIK (non-fraud samples): {total_rows - matched_rows}\")\n",
    "    unique_cik_count = df_result['CIK'].dropna().nunique()\n",
    "    print(f\"Unique CIK count (deduplicated fraud samples): {unique_cik_count}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 6. View sample data\n",
    "    # Print a few rows of data successfully matched to CIK\n",
    "    print(\"\\nSample of successfully matched records (first 5 rows):\")\n",
    "    print(df_result[df_result['CIK'].notna()][['gvkey', 'p_aaer', 'CIK', 'misstate']].head())\n",
    "\n",
    "    # 7. Save results (optional)\n",
    "    df_result.to_csv('C:\\\\Users\\\\MaXin\\\\Desktop\\\\HSBC\\\\FraudDetection-master\\\\FraudDetection\\\\data_with_cik.csv', index=False)\n",
    "    print(\"\\nFile saved as data_with_cik.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_gvkey_to_cik()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761979e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    # Reorder columns to place 'CIK' right after 'gvkey'\n",
    "    cols = list(df_result.columns)\n",
    "    if 'gvkey' in cols and 'CIK' in cols:\n",
    "        cols.remove('CIK')\n",
    "        gvkey_index = cols.index('gvkey')\n",
    "        cols.insert(gvkey_index + 1, 'CIK')\n",
    "    df_result = df_result[cols]\n",
    "    \n",
    "    df_result.to_csv('data_with_cik.csv', index=False)\n",
    "    print(\"\\nFile saved as data_with_cik.csv\")\n",
    "    \n",
    "    # Save fraud-only rows to a new CSV\n",
    "    fraud_df = df_result[df_result['misstate'] == 1]\n",
    "    fraud_df.to_csv('fraud_only_data.csv', index=False)\n",
    "    print(\"Fraud-only data saved as fraud_only_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72366793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def analyze_ticker_matches(df):\n",
    "    # 1. 筛选出成功匹配到 Ticker 的行\n",
    "    # 注意：根据之前的逻辑，没匹配上的可能是 None 或 NaN\n",
    "    mask_matched = df['Ticker'].notna()\n",
    "    df_matched = df[mask_matched]\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 统计 1: 总体匹配情况\n",
    "    # -------------------------------------------------------\n",
    "    total_rows = len(df)\n",
    "    matched_count = len(df_matched)\n",
    "    unique_companies_matched = df_matched['CIK'].nunique()\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(\"DATASET TICKER 匹配统计报告\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"总行数 (Total Rows): {total_rows}\")\n",
    "    print(f\"成功匹配 Ticker 的行数: {matched_count}\")\n",
    "    print(f\"匹配率: {matched_count / total_rows:.2%}\")\n",
    "    print(f\"成功匹配的唯一公司数 (Unique Companies): {unique_companies_matched}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 统计 2: 造假样本 vs 非造假样本 的匹配情况\n",
    "    # -------------------------------------------------------\n",
    "    # 很多老旧的造假公司可能已经退市，导致匹配失败，这里看下差异\n",
    "    if 'misstate' in df.columns:\n",
    "        fraud_df = df[df['misstate'] == 1]\n",
    "        fraud_matched = fraud_df[fraud_df['Ticker'].notna()]\n",
    "        \n",
    "        print(f\"造假样本 (Fraud) 总数: {len(fraud_df)}\")\n",
    "        print(f\"造假样本匹配到 Ticker 数: {len(fraud_matched)}\")\n",
    "        print(f\"造假样本匹配率: {len(fraud_matched) / len(fraud_df) if len(fraud_df)>0 else 0:.2%}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    # 统计 3: 年份分布 (Year Distribution)\n",
    "    # -------------------------------------------------------\n",
    "    print(\"匹配成功的年份分布 (前10个年份):\")\n",
    "    year_dist = df_matched['fyear'].value_counts().sort_index()\n",
    "    print(year_dist.head(10))\n",
    "    print(\"...\")\n",
    "    print(year_dist.tail(5))\n",
    "    \n",
    "    # -------------------------------------------------------\n",
    "    # 可视化 (可选)\n",
    "    # -------------------------------------------------------\n",
    "    # 绘制简单的柱状图看分布\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(year_dist.index, year_dist.values, color='skyblue', label='Matched Rows')\n",
    "    plt.title('Distribution of Rows with Matched Tickers by Fiscal Year')\n",
    "    plt.xlabel('Fiscal Year (fyear)')\n",
    "    plt.ylabel('Count of Matched Rows')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 运行分析\n",
    "analyze_ticker_matches(df_final)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
